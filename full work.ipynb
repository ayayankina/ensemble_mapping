{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9fee0c8-16f6-4949-8c50-88c62a88becc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu121\n",
      "True\n",
      "NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # Display PyTorch version\n",
    "print(torch.cuda.is_available())  # Check if CUDA is available\n",
    "print(torch.cuda.get_device_name(0))  # Get the name of your GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2779b0fd-d7b9-4695-a5b6-be7462c426c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текущий девайс: cuda\n",
      "Матрица успешно умножена на: cuda\n",
      "Shape результата: torch.Size([10000, 10000])\n"
     ]
    }
   ],
   "source": [
    "# Проверим доступное устройство\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Текущий девайс:\", device)\n",
    "\n",
    "# Создаем тензоры\n",
    "a = torch.rand(10000, 10000, device=device)\n",
    "b = torch.rand(10000, 10000, device=device)\n",
    "\n",
    "# Вычисления на GPU\n",
    "c = torch.matmul(a, b)\n",
    "\n",
    "print(\"Матрица успешно умножена на:\", device)\n",
    "print(\"Shape результата:\", c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d521ce9-9ddf-4a6a-80b0-89bc318a9ba6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Скачиваем USPTO-15k\n",
    "!wget https://github.com/wengong-jin/nips17-rexgen/raw/refs/heads/master/USPTO-15K/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6f30de-fc22-4f33-85e8-9809573d4646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "  inflating: data/connor_original.tsv  \n",
      "  inflating: data/test.txt           \n",
      "  inflating: data/train.txt          \n",
      "  inflating: data/valid.txt          \n"
     ]
    }
   ],
   "source": [
    "# распаковываем \n",
    "!unzip data.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c038f1a9-7596-4acd-af8a-6908c0f0b207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Atom-matching-network'...\n",
      "remote: Enumerating objects: 407, done.\u001b[K\n",
      "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
      "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
      "remote: Total 407 (delta 40), reused 11 (delta 11), pack-reused 324 (from 1)\u001b[K\n",
      "Receiving objects: 100% (407/407), 11.04 MiB | 7.11 MiB/s, done.\n",
      "Resolving deltas: 100% (191/191), done.\n"
     ]
    }
   ],
   "source": [
    "# клонируем скрипты с гитхаба\n",
    "!git clone https://github.com/maryamastero/Atom-matching-network.git\n",
    "!cd Atom-matching-network\n",
    "# поменяли название на Atom_matching_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000b06f-500f-4df3-9310-3d3b55ae105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "!python Atom_matching_network/dataset/preprocessing.py \n",
    "# поменяли в скрипте ссылки на файлы с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d732261e-8f38-4c10-8f19-42b553aaf589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])\n",
      "48\n",
      "tensor([28, 45, 35,  9, 11, 38, 40, 46, 41,  5,  3, 14,  8, 32, 42, 43, 30, 44,\n",
      "        18, 29, 24,  4, 25,  1,  2, 23, 13, 36, 16,  0,  6, 19, 21, 15, 39, 10,\n",
      "        22, 37, 33,  7, 47, 17, 31, 12, 27, 34, 20, 26])\n"
     ]
    }
   ],
   "source": [
    "# creating molgraphdataset\n",
    "!python Atom_matching_network/dataset/molgraphdataset.py\n",
    "# поменяли в скрипте ссылки на файлы с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34312927-627c-489d-8e18-936486e60641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9535, 0.0184, 0.0216, 0.0065],\n",
      "        [0.0222, 0.9413, 0.0131, 0.0234],\n",
      "        [0.0194, 0.0097, 0.9358, 0.0351],\n",
      "        [0.0378, 0.1132, 0.2281, 0.6209]], grad_fn=<IndexBackward0>)\n",
      "True\n",
      "tensor(0.3466, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# реализация amnet\n",
    "!python Atom_matching_network/AMNet/amnet.py\n",
    "# корректировка device в скрипте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42f52e98-a3f6-4f4d-a018-9a32beb2b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wl (Тест Вайсфейлера-Лемана)\n",
    "!python Atom_matching_network/utils/wl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c656ca23-ad1f-44a3-be72-df5a8b392087",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "Memory allocated: 0.00 GB\n",
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "FMNet(\n",
      "gnn=GIN(111, 512, num_layers=3, batch_norm=False, cat=True, lin=True),\n",
      "\n",
      "Namespace(num_wl_iterations=3, node_features_dim=111, edge_feature_dim=10, santitize=False, embedding_dim=512, num_layers=3, lr=0.0001, n_epochs=200, batch_size=1)\n",
      "  0%|                                                   | 0/200 [00:00<?, ?it/s]Epoch: 01 *****\n",
      "Train Loss: 0.5147 | Train Acc: 74.72%\n",
      "  0%|▏                                      | 1/200 [05:03<16:47:14, 303.69s/it]Epoch: 02 *****\n",
      "Train Loss: 0.3985 | Train Acc: 77.86%\n",
      "  1%|▍                                      | 2/200 [10:14<16:56:39, 308.08s/it]Epoch: 03 *****\n",
      "Train Loss: 0.3704 | Train Acc: 78.73%\n",
      "  2%|▌                                      | 3/200 [15:23<16:53:07, 308.57s/it]Epoch: 04 *****\n",
      "Train Loss: 0.3582 | Train Acc: 79.06%\n",
      "  2%|▊                                      | 4/200 [20:50<17:10:36, 315.49s/it]Epoch: 05 *****\n",
      "Train Loss: 0.3458 | Train Acc: 79.43%\n",
      "  2%|▉                                      | 5/200 [25:56<16:54:16, 312.08s/it]Epoch: 06 *****\n",
      "Train Loss: 0.3395 | Train Acc: 79.63%\n",
      "  3%|█▏                                     | 6/200 [31:41<17:25:10, 323.25s/it]Epoch: 07 *****\n",
      "Train Loss: 0.3339 | Train Acc: 79.75%\n",
      "  4%|█▎                                     | 7/200 [38:06<18:25:27, 343.67s/it]Epoch: 08 *****\n",
      "Train Loss: 0.3293 | Train Acc: 79.89%\n",
      "  4%|█▌                                     | 8/200 [44:07<18:37:24, 349.19s/it]Epoch: 09 *****\n",
      "Train Loss: 0.3267 | Train Acc: 79.94%\n",
      "  4%|█▊                                     | 9/200 [50:08<18:43:03, 352.79s/it]Epoch: 10 *****\n",
      "Train Loss: 0.3218 | Train Acc: 80.13%\n",
      "  5%|█▉                                    | 10/200 [55:52<18:28:18, 349.99s/it]Epoch: 11 *****\n",
      "Train Loss: 0.3189 | Train Acc: 80.19%\n",
      "  6%|█▉                                  | 11/200 [1:01:33<18:13:54, 347.27s/it]Epoch: 12 *****\n",
      "Train Loss: 0.3168 | Train Acc: 80.27%\n",
      "  6%|██▏                                 | 12/200 [1:07:16<18:04:10, 346.01s/it]Epoch: 13 *****\n",
      "Train Loss: 0.3153 | Train Acc: 80.29%\n",
      "  6%|██▎                                 | 13/200 [1:12:58<17:54:32, 344.77s/it]Epoch: 14 *****\n",
      "Train Loss: 0.3134 | Train Acc: 80.33%\n",
      "  7%|██▌                                 | 14/200 [1:18:40<17:46:24, 344.00s/it]Epoch: 15 *****\n",
      "Train Loss: 0.3116 | Train Acc: 80.43%\n",
      "  8%|██▋                                 | 15/200 [1:24:23<17:39:38, 343.67s/it]Epoch: 16 *****\n",
      "Train Loss: 0.3091 | Train Acc: 80.48%\n",
      "  8%|██▉                                 | 16/200 [1:30:05<17:32:02, 343.06s/it]Epoch: 17 *****\n",
      "Train Loss: 0.3090 | Train Acc: 80.50%\n",
      "  8%|███                                 | 17/200 [1:35:47<17:25:16, 342.72s/it]Epoch: 18 *****\n",
      "Train Loss: 0.3075 | Train Acc: 80.51%\n",
      "  9%|███▏                                | 18/200 [1:41:28<17:18:30, 342.36s/it]Epoch: 19 *****\n",
      "Train Loss: 0.3069 | Train Acc: 80.51%\n",
      " 10%|███▍                                | 19/200 [1:47:09<17:11:34, 341.96s/it]Epoch: 20 *****\n",
      "Train Loss: 0.3055 | Train Acc: 80.56%\n",
      " 10%|███▌                                | 20/200 [1:52:52<17:06:47, 342.26s/it]Epoch: 21 *****\n",
      "Train Loss: 0.3050 | Train Acc: 80.56%\n",
      " 10%|███▊                                | 21/200 [1:58:33<17:00:04, 341.92s/it]Epoch: 22 *****\n",
      "Train Loss: 0.3036 | Train Acc: 80.59%\n",
      " 11%|███▉                                | 22/200 [2:04:14<16:53:32, 341.64s/it]Epoch: 23 *****\n",
      "Train Loss: 0.3034 | Train Acc: 80.60%\n",
      " 12%|████▏                               | 23/200 [2:09:55<16:47:11, 341.42s/it]Epoch: 24 *****\n",
      "Train Loss: 0.3030 | Train Acc: 80.62%\n",
      " 12%|████▎                               | 24/200 [2:15:37<16:41:58, 341.58s/it]Epoch: 25 *****\n",
      "Train Loss: 0.3021 | Train Acc: 80.64%\n",
      " 12%|████▌                               | 25/200 [2:21:26<16:42:36, 343.75s/it]Epoch: 26 *****\n",
      "Train Loss: 0.3015 | Train Acc: 80.66%\n",
      " 13%|████▋                               | 26/200 [2:27:14<16:40:41, 345.07s/it]Epoch: 27 *****\n",
      "Train Loss: 0.3017 | Train Acc: 80.64%\n",
      " 14%|████▊                               | 27/200 [2:33:08<16:42:34, 347.71s/it]Epoch: 28 *****\n",
      "Train Loss: 0.3005 | Train Acc: 80.70%\n",
      " 14%|█████                               | 28/200 [2:38:58<16:38:41, 348.38s/it]Epoch: 29 *****\n",
      "Train Loss: 0.2992 | Train Acc: 80.71%\n",
      " 14%|█████▏                              | 29/200 [2:44:49<16:35:19, 349.24s/it]Epoch: 30 *****\n",
      "Train Loss: 0.2993 | Train Acc: 80.73%\n",
      " 15%|█████▍                              | 30/200 [2:50:42<16:32:28, 350.28s/it]Epoch: 31 *****\n",
      "Train Loss: 0.2980 | Train Acc: 80.73%\n",
      " 16%|█████▌                              | 31/200 [2:56:31<16:25:25, 349.85s/it]Epoch: 32 *****\n",
      "Train Loss: 0.2979 | Train Acc: 80.73%\n",
      " 16%|█████▊                              | 32/200 [3:02:19<16:18:37, 349.51s/it]Epoch: 33 *****\n",
      "Train Loss: 0.2988 | Train Acc: 80.73%\n",
      " 16%|█████▉                              | 33/200 [3:08:15<16:18:19, 351.49s/it]Epoch: 34 *****\n",
      "Train Loss: 0.2973 | Train Acc: 80.76%\n",
      " 17%|██████                              | 34/200 [3:14:05<16:10:44, 350.87s/it]Epoch: 35 *****\n",
      "Train Loss: 0.2976 | Train Acc: 80.74%\n",
      " 18%|██████▎                             | 35/200 [3:19:55<16:04:21, 350.68s/it]Epoch: 36 *****\n",
      "Train Loss: 0.2970 | Train Acc: 80.75%\n",
      " 18%|██████▍                             | 36/200 [3:25:49<16:01:27, 351.75s/it]Epoch: 37 *****\n",
      "Train Loss: 0.2969 | Train Acc: 80.77%\n",
      " 18%|██████▋                             | 37/200 [3:31:39<15:53:41, 351.05s/it]Epoch: 38 *****\n",
      "Train Loss: 0.2961 | Train Acc: 80.77%\n",
      " 19%|██████▊                             | 38/200 [3:37:30<15:47:42, 351.00s/it]Epoch: 39 *****\n",
      "Train Loss: 0.2960 | Train Acc: 80.76%\n",
      " 20%|███████                             | 39/200 [3:43:22<15:42:48, 351.36s/it]Epoch: 40 *****\n",
      "Train Loss: 0.2958 | Train Acc: 80.78%\n",
      " 20%|███████▏                            | 40/200 [3:49:14<15:37:29, 351.56s/it]Epoch: 41 *****\n",
      "Train Loss: 0.2948 | Train Acc: 80.84%\n",
      " 20%|███████▍                            | 41/200 [3:55:05<15:31:02, 351.34s/it]Epoch: 42 *****\n",
      "Train Loss: 0.2946 | Train Acc: 80.83%\n",
      " 21%|███████▌                            | 42/200 [4:00:56<15:25:24, 351.42s/it]Epoch: 43 *****\n",
      "Train Loss: 0.2948 | Train Acc: 80.84%\n",
      " 22%|███████▋                            | 43/200 [4:06:49<15:20:50, 351.91s/it]Epoch: 44 *****\n",
      "Train Loss: 0.2950 | Train Acc: 80.82%\n",
      " 22%|███████▉                            | 44/200 [4:12:40<15:13:35, 351.38s/it]Epoch: 45 *****\n",
      "Train Loss: 0.2938 | Train Acc: 80.84%\n",
      " 22%|████████                            | 45/200 [4:18:32<15:08:26, 351.66s/it]Epoch: 46 *****\n",
      "Train Loss: 0.2937 | Train Acc: 80.87%\n",
      " 23%|████████▎                           | 46/200 [4:24:29<15:06:38, 353.23s/it]Epoch: 47 *****\n",
      "Train Loss: 0.2940 | Train Acc: 80.87%\n",
      " 24%|████████▍                           | 47/200 [4:30:22<15:00:54, 353.30s/it]Epoch: 48 *****\n",
      "Train Loss: 0.2942 | Train Acc: 80.83%\n",
      " 24%|████████▋                           | 48/200 [4:36:18<14:57:16, 354.19s/it]Epoch: 49 *****\n",
      "Train Loss: 0.2933 | Train Acc: 80.88%\n",
      " 24%|████████▊                           | 49/200 [4:42:12<14:50:33, 353.86s/it]Epoch: 50 *****\n",
      "Train Loss: 0.2925 | Train Acc: 80.85%\n",
      " 25%|█████████                           | 50/200 [4:48:10<14:47:46, 355.11s/it]Epoch: 51 *****\n",
      "Train Loss: 0.2934 | Train Acc: 80.88%\n",
      " 26%|█████████▏                          | 51/200 [4:54:04<14:41:42, 355.05s/it]Epoch: 52 *****\n",
      "Train Loss: 0.2922 | Train Acc: 80.88%\n",
      " 26%|█████████▎                          | 52/200 [5:00:06<14:40:48, 357.08s/it]Epoch: 53 *****\n",
      "Train Loss: 0.2925 | Train Acc: 80.88%\n",
      " 26%|█████████▌                          | 53/200 [5:06:00<14:32:36, 356.17s/it]Epoch: 54 *****\n",
      "Train Loss: 0.2925 | Train Acc: 80.86%\n",
      " 27%|█████████▋                          | 54/200 [5:11:55<14:25:32, 355.70s/it]Epoch: 55 *****\n",
      "Train Loss: 0.2923 | Train Acc: 80.89%\n",
      " 28%|█████████▉                          | 55/200 [5:17:52<14:20:13, 355.95s/it]Epoch: 56 *****\n",
      "Train Loss: 0.2915 | Train Acc: 80.91%\n",
      " 28%|██████████                          | 56/200 [5:23:45<14:12:19, 355.13s/it]Epoch: 57 *****\n",
      "Train Loss: 0.2912 | Train Acc: 80.88%\n",
      " 28%|██████████▎                         | 57/200 [5:29:41<14:06:52, 355.33s/it]Epoch: 58 *****\n",
      "Train Loss: 0.2919 | Train Acc: 80.87%\n",
      " 29%|██████████▍                         | 58/200 [5:35:36<14:01:13, 355.45s/it]Epoch: 59 *****\n",
      "Train Loss: 0.2917 | Train Acc: 80.88%\n",
      " 30%|██████████▌                         | 59/200 [5:41:34<13:56:59, 356.17s/it]Epoch: 60 *****\n",
      "Train Loss: 0.2909 | Train Acc: 80.88%\n",
      " 30%|██████████▊                         | 60/200 [5:47:30<13:50:57, 356.12s/it]Epoch: 61 *****\n",
      "Train Loss: 0.2916 | Train Acc: 80.91%\n",
      " 30%|██████████▉                         | 61/200 [5:53:29<13:46:40, 356.84s/it]Epoch: 62 *****\n",
      "Train Loss: 0.2914 | Train Acc: 80.86%\n",
      " 31%|███████████▏                        | 62/200 [5:59:24<13:39:45, 356.42s/it]Epoch: 63 *****\n",
      "Train Loss: 0.2912 | Train Acc: 80.90%\n",
      " 32%|███████████▎                        | 63/200 [6:05:09<13:26:03, 353.02s/it]Epoch: 64 *****\n",
      "Train Loss: 0.2905 | Train Acc: 80.91%\n",
      " 32%|███████████▌                        | 64/200 [6:10:52<13:13:11, 349.94s/it]Epoch: 65 *****\n",
      "Train Loss: 0.2900 | Train Acc: 80.92%\n",
      " 32%|███████████▋                        | 65/200 [6:16:34<13:01:53, 347.50s/it]Epoch: 66 *****\n",
      "Train Loss: 0.2896 | Train Acc: 80.95%\n",
      " 33%|███████████▉                        | 66/200 [6:22:16<12:52:16, 345.79s/it]Epoch: 67 *****\n",
      "Train Loss: 0.2907 | Train Acc: 80.92%\n",
      " 34%|████████████                        | 67/200 [6:28:07<12:50:11, 347.46s/it]Epoch: 68 *****\n",
      "Train Loss: 0.2906 | Train Acc: 80.94%\n",
      " 34%|████████████▏                       | 68/200 [6:33:51<12:41:59, 346.36s/it]Epoch: 69 *****\n",
      "Train Loss: 0.2906 | Train Acc: 80.94%\n",
      " 34%|████████████▍                       | 69/200 [6:39:30<12:31:49, 344.35s/it]Epoch: 70 *****\n",
      "Train Loss: 0.2912 | Train Acc: 80.91%\n",
      " 35%|████████████▌                       | 70/200 [6:45:10<12:23:13, 343.02s/it]Epoch: 71 *****\n",
      "Train Loss: 0.2899 | Train Acc: 80.94%\n",
      " 36%|████████████▊                       | 71/200 [6:50:59<12:21:00, 344.65s/it]Epoch: 72 *****\n",
      "Train Loss: 0.2899 | Train Acc: 80.93%\n",
      " 36%|████████████▉                       | 72/200 [6:56:39<12:12:34, 343.39s/it]Epoch: 73 *****\n",
      "Train Loss: 0.2905 | Train Acc: 80.92%\n",
      " 36%|█████████████▏                      | 73/200 [7:02:19<12:04:43, 342.39s/it]Epoch: 74 *****\n",
      "Train Loss: 0.2903 | Train Acc: 80.92%\n",
      " 37%|█████████████▎                      | 74/200 [7:07:59<11:57:31, 341.68s/it]Epoch: 75 *****\n",
      "Train Loss: 0.2902 | Train Acc: 80.91%\n",
      " 38%|█████████████▌                      | 75/200 [7:13:40<11:51:26, 341.49s/it]Epoch: 76 *****\n",
      "Train Loss: 0.2893 | Train Acc: 80.94%\n",
      " 38%|█████████████▋                      | 76/200 [7:19:24<11:47:02, 342.12s/it]Epoch: 77 *****\n",
      "Train Loss: 0.2894 | Train Acc: 80.94%\n",
      " 38%|█████████████▊                      | 77/200 [7:25:06<11:41:27, 342.18s/it]Epoch: 78 *****\n",
      "Train Loss: 0.2892 | Train Acc: 80.95%\n",
      " 39%|██████████████                      | 78/200 [7:30:45<11:33:54, 341.27s/it]Epoch: 79 *****\n",
      "Train Loss: 0.2890 | Train Acc: 80.96%\n",
      " 40%|██████████████▏                     | 79/200 [7:36:28<11:28:49, 341.57s/it]Epoch: 80 *****\n",
      "Train Loss: 0.2896 | Train Acc: 80.95%\n",
      " 40%|██████████████▍                     | 80/200 [7:42:09<11:22:55, 341.47s/it]Epoch: 81 *****\n",
      "Train Loss: 0.2891 | Train Acc: 80.96%\n",
      " 40%|██████████████▌                     | 81/200 [7:47:50<11:17:13, 341.46s/it]Epoch: 82 *****\n",
      "Train Loss: 0.2894 | Train Acc: 80.96%\n",
      " 41%|██████████████▊                     | 82/200 [7:53:32<11:11:50, 341.62s/it]Epoch: 83 *****\n",
      "Train Loss: 0.2894 | Train Acc: 80.95%\n",
      " 42%|██████████████▉                     | 83/200 [7:59:11<11:04:46, 340.91s/it]Epoch: 84 *****\n",
      "Train Loss: 0.2882 | Train Acc: 80.96%\n",
      " 42%|███████████████                     | 84/200 [8:04:50<10:57:39, 340.17s/it]Epoch: 85 *****\n",
      "Train Loss: 0.2882 | Train Acc: 80.98%\n",
      " 42%|███████████████▎                    | 85/200 [8:10:37<10:56:02, 342.28s/it]Epoch: 86 *****\n",
      "Train Loss: 0.2887 | Train Acc: 80.96%\n",
      " 43%|███████████████▍                    | 86/200 [8:16:18<10:49:45, 341.98s/it]Epoch: 87 *****\n",
      "Train Loss: 0.2889 | Train Acc: 80.95%\n",
      " 44%|███████████████▋                    | 87/200 [8:21:58<10:42:36, 341.20s/it]Epoch: 88 *****\n",
      "Train Loss: 0.2885 | Train Acc: 80.99%\n",
      " 44%|███████████████▊                    | 88/200 [8:27:38<10:36:27, 340.96s/it]Epoch: 89 *****\n",
      "Train Loss: 0.2880 | Train Acc: 80.98%\n",
      " 44%|████████████████                    | 89/200 [8:33:20<10:31:06, 341.14s/it]Epoch: 90 *****\n",
      "Train Loss: 0.2885 | Train Acc: 80.96%\n",
      " 45%|████████████████▏                   | 90/200 [8:39:01<10:25:40, 341.28s/it]Epoch: 91 *****\n",
      "Train Loss: 0.2878 | Train Acc: 80.99%\n",
      " 46%|████████████████▍                   | 91/200 [8:44:50<10:23:52, 343.41s/it]Epoch: 92 *****\n",
      "Train Loss: 0.2878 | Train Acc: 80.96%\n",
      " 46%|████████████████▌                   | 92/200 [8:50:31<10:16:43, 342.62s/it]Epoch: 93 *****\n",
      "Train Loss: 0.2877 | Train Acc: 80.99%\n",
      " 46%|████████████████▋                   | 93/200 [8:56:11<10:09:54, 342.01s/it]Epoch: 94 *****\n",
      "Train Loss: 0.2886 | Train Acc: 80.98%\n",
      " 47%|████████████████▉                   | 94/200 [9:01:51<10:03:16, 341.47s/it]Epoch: 95 *****\n",
      "Train Loss: 0.2884 | Train Acc: 80.94%\n",
      " 48%|█████████████████▌                   | 95/200 [9:07:33<9:57:30, 341.44s/it]Epoch: 96 *****\n",
      "Train Loss: 0.2886 | Train Acc: 80.98%\n",
      " 48%|█████████████████▊                   | 96/200 [9:13:17<9:53:23, 342.34s/it]Epoch: 97 *****\n",
      "Train Loss: 0.2881 | Train Acc: 80.99%\n",
      " 48%|█████████████████▉                   | 97/200 [9:18:57<9:46:22, 341.57s/it]Epoch: 98 *****\n",
      "Train Loss: 0.2884 | Train Acc: 80.97%\n",
      " 49%|██████████████████▏                  | 98/200 [9:24:41<9:41:55, 342.31s/it]Epoch: 99 *****\n",
      "Train Loss: 0.2877 | Train Acc: 81.00%\n",
      "Early stopping. No improvement in 99 epochs.\n",
      " 49%|██████████████████▏                  | 98/200 [9:30:24<9:53:41, 349.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# amnet_train\n",
    "!python Atom_matching_network/train/amnet_train.py\n",
    "# исправили проблему импорта модулей, добавили подробный логгер в скрипт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aac75c52-bc1b-4514-a780-801a99e32328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "cuda\n",
      "FMNet(\n",
      "gnn=GIN(111, 512, num_layers=3, batch_norm=False, cat=True, lin=True),\n",
      "\n",
      "Namespace(num_wl_iterations=3, node_features_dim=111, edge_feature_dim=None, santitize=False, embedding_dim=512, num_layers=3, lr=0.001, batch_size=1)\n",
      "[{0, 9}, {1}, {2}, {3}, {4}, {5}, {6}, {7}, {8}, {10, 12}, {11}, {13}, {14}, {15}, {16}, {17}, {18}]\n",
      "********************\n",
      "acc: 0.6842105263157895\n",
      "h1: 0.5789473684210527\n",
      "h3: 1.0\n",
      "h5: 1.0\n",
      "h10: 1.0\n",
      "0.9010107670441664\n",
      "gt, atom mapping tensor([17, 11, 13, 18,  2,  4,  7, 15, 16,  3, 10, 12,  8,  1,  0,  5,  9,  6,\n",
      "        14], device='cuda:0')\n",
      "pred, atom mapping [15, 7, 13, 4, 13, 4, 7, 15, 1, 3, 12, 10, 3, 1, 9, 5, 0, 6, 14]\n"
     ]
    }
   ],
   "source": [
    "!python Atom_matching_network/Test/test_one_mol.py\n",
    "# корректировка device, исправили проблему импорта модулей в скрипте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec70a2c7-8892-41fe-b6aa-2ec2f7ff1d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "cuda\n",
      "FMNet(\n",
      "gnn=GIN(111, 512, num_layers=3, batch_norm=False, cat=True, lin=True),\n",
      "\n",
      "Namespace(num_wl_iterations=3, node_features_dim=111, edge_feature_dim=None, santitize=False, embedding_dim=512, num_layers=3, lr=0.001, batch_size=1)\n",
      "[{0}, {1}, {2}, {3}, {4}, {5}, {10, 6}, {7}, {8}, {9}, {11}, {12}, {13}, {14}, {15}]\n",
      "********************\n",
      "acc: 0.625\n",
      "h1: 0.5625\n",
      "h3: 1.0\n",
      "h5: 1.0\n",
      "h10: 1.0\n",
      "0.8333826059048329\n",
      "gt, atom mapping tensor([14,  8,  9, 15, 12,  2,  3,  0,  7,  1,  5, 13,  6, 10,  4, 11],\n",
      "       device='cuda:0')\n",
      "pred, atom mapping [14, 8, 8, 14, 12, 2, 7, 0, 7, 2, 11, 4, 10, 6, 4, 11]\n"
     ]
    }
   ],
   "source": [
    "# тест другой молекулы\n",
    "!python Atom_matching_network/Test/test_one_mol.py\n",
    "# корректировка device, исправили проблему импорта модулей в скрипте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c369165d-7f80-4aea-af6c-166f1971deb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "cuda\n",
      "FMNet(\n",
      "gnn=GIN(111, 512, num_layers=3, batch_norm=False, cat=True, lin=True),\n",
      "\n",
      "Namespace(num_wl_iterations=3, node_features_dim=111, edge_feature_dim=None, santitize=False, embedding_dim=512, num_layers=3, lr=0.001, batch_size=1)\n",
      "[{0, 11}, {1}, {2}, {3}, {4, 13}, {5}, {6}, {7}, {8}, {9, 18, 28}, {10}, {12}, {14}, {15}, {16, 25}, {17}, {19}, {20}, {21}, {22}, {23}, {24}, {26}, {33, 27}, {29}, {30}, {31}, {32}, {34}]\n",
      "********************\n",
      "acc: 1.0\n",
      "h1: 0.8571428571428571\n",
      "h3: 1.0\n",
      "h5: 1.0\n",
      "h10: 1.0\n",
      "0.9326219740492461\n",
      "gt, atom mapping tensor([19, 33, 23, 27, 29,  2, 17, 34, 14,  0, 11, 12,  1, 21, 10, 31,  7, 32,\n",
      "        22, 15,  3, 26,  5, 28,  9, 18,  8, 13, 25,  6, 16,  4, 24, 20, 30],\n",
      "       device='cuda:0')\n",
      "pred, atom mapping [19, 27, 23, 27, 29, 2, 17, 34, 14, 11, 0, 12, 1, 21, 10, 31, 7, 32, 22, 15, 3, 26, 5, 18, 9, 9, 8, 13, 25, 6, 16, 4, 24, 20, 30]\n"
     ]
    }
   ],
   "source": [
    "# тест другой молекулы\n",
    "!python Atom_matching_network/Test/test_one_mol.py\n",
    "# корректировка device, исправили проблему импорта модулей в скрипте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a486d3c-47d1-4b6e-8764-bf3d753bb37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "cuda\n",
      "FMNet(\n",
      "gnn=GIN(111, 512, num_layers=3, batch_norm=False, cat=True, lin=True),\n",
      "\n",
      "Namespace(num_wl_iterations=3, node_features_dim=111, edge_feature_dim=10, santitize=False, embedding_dim=512, num_layers=3, lr=0.001, batch_size=1)\n",
      "********************\n",
      "Средняя точность модели: 0.9687 (96.87%)\n",
      "Медианная точность: 1.0000\n",
      "Стандартное отклонение: 0.0531\n",
      "Минимальная точность: 0.6250\n",
      "Максимальная точность: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# тест целого батча\n",
    "!python Atom_matching_network/Test/test_batch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccfe9e8-5365-40b8-9713-cab766b13abb",
   "metadata": {},
   "source": [
    "## МОЙ ПРОЕКТ ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772b6d2-ff14-4551-86b0-a97897df3eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/wengong-jin/nips17-rexgen/raw/refs/heads/master/USPTO-15K/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5a45b29-446e-4387-803b-43e17665077a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "  inflating: data/connor_original.tsv  \n",
      "  inflating: data/test.txt           \n",
      "  inflating: data/train.txt          \n",
      "  inflating: data/valid.txt          \n"
     ]
    }
   ],
   "source": [
    "!unzip data.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc0758a1-2cf7-431c-9893-c212c5a9bee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Atom-matching-network'...\n",
      "remote: Enumerating objects: 407, done.\u001b[K\n",
      "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
      "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
      "remote: Total 407 (delta 40), reused 11 (delta 11), pack-reused 324 (from 1)\u001b[K\n",
      "Receiving objects: 100% (407/407), 11.04 MiB | 7.11 MiB/s, done.\n",
      "Resolving deltas: 100% (191/191), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/maryamastero/Atom-matching-network.git\n",
    "!cd Atom-matching-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acdd70c-86aa-4ab9-8e08-dcd4ca4ee2dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "!python Atom_matching_network/dataset/preprocessing.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57a113e6-d460-4118-ba3c-eff8a1194047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])\n",
      "48\n",
      "tensor([28, 45, 35,  9, 11, 38, 40, 46, 41,  5,  3, 14,  8, 32, 42, 43, 30, 44,\n",
      "        18, 29, 24,  4, 25,  1,  2, 23, 13, 36, 16,  0,  6, 19, 21, 15, 39, 10,\n",
      "        22, 37, 33,  7, 47, 17, 31, 12, 27, 34, 20, 26])\n"
     ]
    }
   ],
   "source": [
    "# creating molgraphdataset\n",
    "!python Atom_matching_network/dataset/molgraphdataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "632471dd-f669-4dae-9b13-ee3ddd3a51b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fold 0: saved 5973 train / 1494 val\n",
      "✅ Fold 1: saved 5973 train / 1494 val\n",
      "✅ Fold 2: saved 5974 train / 1493 val\n",
      "✅ Fold 3: saved 5974 train / 1493 val\n",
      "✅ Fold 4: saved 5974 train / 1493 val\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "# Путь к файлам\n",
    "data_dir = \"SanitizeMol_data\"\n",
    "folds_dir = os.path.join(data_dir, \"folds\")\n",
    "os.makedirs(folds_dir, exist_ok=True)\n",
    "\n",
    "# Загружаем данные с двумя колонками\n",
    "train_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"), sep=\",\", header=None)\n",
    "valid_df = pd.read_csv(os.path.join(data_dir, \"valid.csv\"), sep=\",\", header=None)\n",
    "\n",
    "# Объединяем train и valid для фолдов\n",
    "full_df = pd.concat([train_df, valid_df], ignore_index=True)\n",
    "\n",
    "# Кол-во фолдов\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Разбиваем и сохраняем фолды\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(full_df)):\n",
    "    train_fold = full_df.iloc[train_idx]\n",
    "    val_fold = full_df.iloc[val_idx]\n",
    "\n",
    "    train_path = os.path.join(folds_dir, f\"train_fold{fold}.csv\")\n",
    "    val_path   = os.path.join(folds_dir, f\"val_fold{fold}.csv\")\n",
    "\n",
    "    train_fold.to_csv(train_path, sep=\",\", header=False, index=False)\n",
    "    val_fold.to_csv(val_path,   sep=\",\", header=False, index=False)\n",
    "\n",
    "    print(f\"✅ Fold {fold}: saved {len(train_fold)} train / {len(val_fold)} val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d988f638-da06-477d-b0a9-f0eace5057c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9535, 0.0184, 0.0216, 0.0065],\n",
      "        [0.0222, 0.9413, 0.0131, 0.0234],\n",
      "        [0.0194, 0.0097, 0.9358, 0.0351],\n",
      "        [0.0378, 0.1132, 0.2281, 0.6209]], grad_fn=<IndexBackward0>)\n",
      "True\n",
      "tensor(0.3466, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# реализация amnet\n",
    "!python Atom_matching_network/AMNet/amnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffc26b5c-bd8f-4ce0-ae6f-40bcdf06936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wl (Тест Вайсфейлера-Лемана)\n",
    "!python Atom_matching_network/utils/wl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a098b6a-b681-4afc-b53b-db4c7806bae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "Memory allocated: 0.00 GB\n",
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "FMNet(\n",
      "gnn=GIN(111, 512, num_layers=3, batch_norm=False, cat=True, lin=True),\n",
      "\n",
      "Namespace(num_wl_iterations=3, node_features_dim=111, edge_feature_dim=10, santitize=False, embedding_dim=512, num_layers=3, lr=0.0001, n_epochs=80, batch_size=1, fold=0)\n",
      "  0%|                                                    | 0/80 [00:00<?, ?it/s]Epoch: 01 *****\n",
      "Train Loss: 0.5327 | Train Acc: 74.19%\n",
      "  1%|▌                                        | 1/80 [02:57<3:53:10, 177.09s/it]Epoch: 02 *****\n",
      "Train Loss: 0.4029 | Train Acc: 77.74%\n",
      "  2%|█                                        | 2/80 [05:50<3:47:23, 174.92s/it]Epoch: 03 *****\n",
      "Train Loss: 0.3762 | Train Acc: 78.52%\n",
      "  4%|█▌                                       | 3/80 [08:44<3:43:42, 174.32s/it]Epoch: 04 *****\n",
      "Train Loss: 0.3597 | Train Acc: 78.97%\n",
      "  5%|██                                       | 4/80 [11:39<3:41:16, 174.69s/it]Epoch: 05 *****\n",
      "Train Loss: 0.3526 | Train Acc: 79.23%\n",
      "  6%|██▌                                      | 5/80 [14:35<3:38:54, 175.13s/it]Epoch: 06 *****\n",
      "Train Loss: 0.3441 | Train Acc: 79.46%\n",
      "  8%|███                                      | 6/80 [17:32<3:37:02, 175.98s/it]Epoch: 07 *****\n",
      "Train Loss: 0.3379 | Train Acc: 79.57%\n",
      "  9%|███▌                                     | 7/80 [20:32<3:35:34, 177.19s/it]Epoch: 08 *****\n",
      "Train Loss: 0.3324 | Train Acc: 79.77%\n",
      " 10%|████                                     | 8/80 [23:30<3:32:51, 177.38s/it]Epoch: 09 *****\n",
      "Train Loss: 0.3293 | Train Acc: 79.85%\n",
      " 11%|████▌                                    | 9/80 [26:29<3:30:22, 177.78s/it]Epoch: 10 *****\n",
      "Train Loss: 0.3262 | Train Acc: 79.94%\n",
      " 12%|█████                                   | 10/80 [29:26<3:27:13, 177.62s/it]Epoch: 11 *****\n",
      "Train Loss: 0.3232 | Train Acc: 80.01%\n",
      " 14%|█████▌                                  | 11/80 [32:24<3:24:36, 177.92s/it]Epoch: 12 *****\n",
      "Train Loss: 0.3204 | Train Acc: 80.09%\n",
      " 15%|██████                                  | 12/80 [35:23<3:22:00, 178.24s/it]Epoch: 13 *****\n",
      "Train Loss: 0.3185 | Train Acc: 80.12%\n",
      " 16%|██████▌                                 | 13/80 [38:22<3:19:13, 178.41s/it]Epoch: 14 *****\n",
      "Train Loss: 0.3155 | Train Acc: 80.19%\n",
      " 18%|███████                                 | 14/80 [41:21<3:16:32, 178.67s/it]Epoch: 15 *****\n",
      "Train Loss: 0.3143 | Train Acc: 80.29%\n",
      " 19%|███████▌                                | 15/80 [44:20<3:13:27, 178.57s/it]Epoch: 16 *****\n",
      "Train Loss: 0.3128 | Train Acc: 80.24%\n",
      " 20%|████████                                | 16/80 [47:19<3:10:34, 178.67s/it]Epoch: 17 *****\n",
      "Train Loss: 0.3113 | Train Acc: 80.35%\n",
      " 21%|████████▌                               | 17/80 [50:17<3:07:36, 178.67s/it]Epoch: 18 *****\n",
      "Train Loss: 0.3103 | Train Acc: 80.35%\n",
      " 22%|█████████                               | 18/80 [53:16<3:04:39, 178.70s/it]Epoch: 19 *****\n",
      "Train Loss: 0.3101 | Train Acc: 80.38%\n",
      " 24%|█████████▌                              | 19/80 [56:14<3:01:26, 178.47s/it]Epoch: 20 *****\n",
      "Train Loss: 0.3076 | Train Acc: 80.41%\n",
      " 25%|██████████                              | 20/80 [59:12<2:58:23, 178.39s/it]Epoch: 21 *****\n",
      "Train Loss: 0.3069 | Train Acc: 80.44%\n",
      " 26%|█████████▉                            | 21/80 [1:02:10<2:55:21, 178.33s/it]Epoch: 22 *****\n",
      "Train Loss: 0.3061 | Train Acc: 80.50%\n",
      " 28%|██████████▍                           | 22/80 [1:05:09<2:52:33, 178.51s/it]Epoch: 23 *****\n",
      "Train Loss: 0.3051 | Train Acc: 80.51%\n",
      " 29%|██████████▉                           | 23/80 [1:08:08<2:49:32, 178.46s/it]Epoch: 24 *****\n",
      "Train Loss: 0.3032 | Train Acc: 80.57%\n",
      " 30%|███████████▍                          | 24/80 [1:11:07<2:46:40, 178.58s/it]Epoch: 25 *****\n",
      "Train Loss: 0.3045 | Train Acc: 80.50%\n",
      " 31%|███████████▉                          | 25/80 [1:14:05<2:43:46, 178.66s/it]Epoch: 26 *****\n",
      "Train Loss: 0.3036 | Train Acc: 80.53%\n",
      " 32%|████████████▎                         | 26/80 [1:17:04<2:40:45, 178.62s/it]Epoch: 27 *****\n",
      "Train Loss: 0.3030 | Train Acc: 80.56%\n",
      " 34%|████████████▊                         | 27/80 [1:20:03<2:37:49, 178.67s/it]Epoch: 28 *****\n",
      "Train Loss: 0.3014 | Train Acc: 80.57%\n",
      " 35%|█████████████▎                        | 28/80 [1:23:01<2:34:45, 178.56s/it]Epoch: 29 *****\n",
      "Train Loss: 0.3023 | Train Acc: 80.60%\n",
      " 36%|█████████████▊                        | 29/80 [1:25:59<2:31:37, 178.37s/it]Epoch: 30 *****\n",
      "Train Loss: 0.3009 | Train Acc: 80.59%\n",
      " 38%|██████████████▎                       | 30/80 [1:28:57<2:28:35, 178.31s/it]Epoch: 31 *****\n",
      "Train Loss: 0.2999 | Train Acc: 80.60%\n",
      " 39%|██████████████▋                       | 31/80 [1:31:56<2:25:49, 178.56s/it]Epoch: 32 *****\n",
      "Train Loss: 0.3009 | Train Acc: 80.60%\n",
      " 40%|███████████████▏                      | 32/80 [1:34:55<2:22:57, 178.69s/it]Epoch: 33 *****\n",
      "Train Loss: 0.2997 | Train Acc: 80.67%\n",
      " 41%|███████████████▋                      | 33/80 [1:37:54<2:19:59, 178.71s/it]Epoch: 34 *****\n",
      "Train Loss: 0.2991 | Train Acc: 80.63%\n",
      " 42%|████████████████▏                     | 34/80 [1:40:53<2:17:05, 178.82s/it]Epoch: 35 *****\n",
      "Train Loss: 0.2991 | Train Acc: 80.62%\n",
      " 44%|████████████████▋                     | 35/80 [1:43:52<2:14:07, 178.83s/it]Epoch: 36 *****\n",
      "Train Loss: 0.2983 | Train Acc: 80.68%\n",
      " 45%|█████████████████                     | 36/80 [1:46:51<2:11:14, 178.97s/it]Epoch: 37 *****\n",
      "Train Loss: 0.2984 | Train Acc: 80.66%\n",
      " 46%|█████████████████▌                    | 37/80 [1:49:51<2:08:24, 179.16s/it]Epoch: 38 *****\n",
      "Train Loss: 0.2978 | Train Acc: 80.67%\n",
      " 48%|██████████████████                    | 38/80 [1:52:50<2:05:22, 179.12s/it]Epoch: 39 *****\n",
      "Train Loss: 0.2971 | Train Acc: 80.68%\n",
      " 49%|██████████████████▌                   | 39/80 [1:55:49<2:02:17, 178.97s/it]Epoch: 40 *****\n",
      "Train Loss: 0.2974 | Train Acc: 80.67%\n",
      " 50%|███████████████████                   | 40/80 [1:58:47<1:59:18, 178.97s/it]Epoch: 41 *****\n",
      "Train Loss: 0.2972 | Train Acc: 80.70%\n",
      " 51%|███████████████████▍                  | 41/80 [2:01:46<1:56:14, 178.82s/it]Epoch: 42 *****\n",
      "Train Loss: 0.2963 | Train Acc: 80.71%\n",
      " 52%|███████████████████▉                  | 42/80 [2:04:45<1:53:18, 178.92s/it]Epoch: 43 *****\n",
      "Train Loss: 0.2969 | Train Acc: 80.72%\n",
      " 54%|████████████████████▍                 | 43/80 [2:07:44<1:50:20, 178.94s/it]Epoch: 44 *****\n",
      "Train Loss: 0.2957 | Train Acc: 80.74%\n",
      " 55%|████████████████████▉                 | 44/80 [2:10:43<1:47:26, 179.07s/it]Epoch: 45 *****\n",
      "Train Loss: 0.2963 | Train Acc: 80.72%\n",
      " 56%|█████████████████████▍                | 45/80 [2:13:43<1:44:27, 179.07s/it]Epoch: 46 *****\n",
      "Train Loss: 0.2957 | Train Acc: 80.72%\n",
      " 57%|█████████████████████▊                | 46/80 [2:16:41<1:41:26, 179.01s/it]Epoch: 47 *****\n",
      "Train Loss: 0.2953 | Train Acc: 80.76%\n",
      " 59%|██████████████████████▎               | 47/80 [2:19:42<1:38:38, 179.34s/it]Epoch: 48 *****\n",
      "Train Loss: 0.2952 | Train Acc: 80.72%\n",
      " 60%|██████████████████████▊               | 48/80 [2:22:41<1:35:39, 179.37s/it]Epoch: 49 *****\n",
      "Train Loss: 0.2949 | Train Acc: 80.74%\n",
      " 61%|███████████████████████▎              | 49/80 [2:25:41<1:32:43, 179.45s/it]Epoch: 50 *****\n",
      "Train Loss: 0.2951 | Train Acc: 80.75%\n",
      " 62%|███████████████████████▊              | 50/80 [2:28:40<1:29:41, 179.39s/it]Epoch: 51 *****\n",
      "Train Loss: 0.2948 | Train Acc: 80.73%\n",
      " 64%|████████████████████████▏             | 51/80 [2:31:39<1:26:39, 179.29s/it]Epoch: 52 *****\n",
      "Train Loss: 0.2939 | Train Acc: 80.79%\n",
      " 65%|████████████████████████▋             | 52/80 [2:34:38<1:23:41, 179.34s/it]Epoch: 53 *****\n",
      "Train Loss: 0.2940 | Train Acc: 80.78%\n",
      " 66%|█████████████████████████▏            | 53/80 [2:37:37<1:20:39, 179.25s/it]Epoch: 54 *****\n",
      "Train Loss: 0.2947 | Train Acc: 80.77%\n",
      " 68%|█████████████████████████▋            | 54/80 [2:40:37<1:17:41, 179.30s/it]Epoch: 55 *****\n",
      "Train Loss: 0.2947 | Train Acc: 80.74%\n",
      " 69%|██████████████████████████▏           | 55/80 [2:43:36<1:14:40, 179.22s/it]Epoch: 56 *****\n",
      "Train Loss: 0.2927 | Train Acc: 80.80%\n",
      " 70%|██████████████████████████▌           | 56/80 [2:46:36<1:11:46, 179.43s/it]Epoch: 57 *****\n",
      "Train Loss: 0.2943 | Train Acc: 80.79%\n",
      " 71%|███████████████████████████           | 57/80 [2:49:35<1:08:47, 179.44s/it]Epoch: 58 *****\n",
      "Train Loss: 0.2950 | Train Acc: 80.77%\n",
      " 72%|███████████████████████████▌          | 58/80 [2:52:35<1:05:51, 179.61s/it]Epoch: 59 *****\n",
      "Train Loss: 0.2933 | Train Acc: 80.76%\n",
      " 74%|████████████████████████████          | 59/80 [2:55:35<1:02:54, 179.74s/it]Epoch: 60 *****\n",
      "Train Loss: 0.2930 | Train Acc: 80.79%\n",
      " 75%|██████████████████████████████          | 60/80 [2:58:34<59:47, 179.38s/it]Epoch: 61 *****\n",
      "Train Loss: 0.2926 | Train Acc: 80.80%\n",
      " 76%|██████████████████████████████▌         | 61/80 [3:01:32<56:43, 179.12s/it]Epoch: 62 *****\n",
      "Train Loss: 0.2926 | Train Acc: 80.78%\n",
      " 78%|███████████████████████████████         | 62/80 [3:04:32<53:48, 179.39s/it]Epoch: 63 *****\n",
      "Train Loss: 0.2920 | Train Acc: 80.82%\n",
      " 79%|███████████████████████████████▌        | 63/80 [3:07:32<50:49, 179.37s/it]Epoch: 64 *****\n",
      "Train Loss: 0.2931 | Train Acc: 80.79%\n",
      " 80%|████████████████████████████████        | 64/80 [3:10:30<47:45, 179.09s/it]Epoch: 65 *****\n",
      "Train Loss: 0.2931 | Train Acc: 80.81%\n",
      " 81%|████████████████████████████████▌       | 65/80 [3:13:29<44:47, 179.15s/it]Epoch: 66 *****\n",
      "Train Loss: 0.2917 | Train Acc: 80.82%\n",
      " 82%|█████████████████████████████████       | 66/80 [3:16:29<41:51, 179.39s/it]Epoch: 67 *****\n",
      "Train Loss: 0.2927 | Train Acc: 80.78%\n",
      " 84%|█████████████████████████████████▌      | 67/80 [3:19:29<38:54, 179.55s/it]Epoch: 68 *****\n",
      "Train Loss: 0.2914 | Train Acc: 80.82%\n",
      "Early stopping. No improvement in 68 epochs.\n",
      " 84%|█████████████████████████████████▌      | 67/80 [3:22:30<39:17, 181.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# amnet_train\n",
    "!python Atom_matching_network/train/amnet_train_fold.py --fold 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d406316a-bb41-4562-a2a1-6cc2d13b0722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "Memory allocated: 0.00 GB\n",
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "FMNet(\n",
      "gnn=GIN(111, 512, num_layers=3, batch_norm=False, cat=True, lin=True),\n",
      "\n",
      "Namespace(num_wl_iterations=3, node_features_dim=111, edge_feature_dim=10, santitize=False, embedding_dim=512, num_layers=3, lr=0.0001, n_epochs=80, batch_size=1, fold=1)\n",
      "  0%|                                                    | 0/80 [00:00<?, ?it/s]Epoch: 01 *****\n",
      "Train Loss: 0.5327 | Train Acc: 74.26%\n",
      "  1%|▌                                        | 1/80 [02:58<3:54:39, 178.22s/it]Epoch: 02 *****\n",
      "Train Loss: 0.4040 | Train Acc: 77.73%\n",
      "  2%|█                                        | 2/80 [05:55<3:50:39, 177.43s/it]Epoch: 03 *****\n",
      "Train Loss: 0.3759 | Train Acc: 78.59%\n",
      "  4%|█▌                                       | 3/80 [08:51<3:47:13, 177.05s/it]Epoch: 04 *****\n",
      "Train Loss: 0.3612 | Train Acc: 79.02%\n",
      "  5%|██                                       | 4/80 [11:49<3:44:40, 177.38s/it]Epoch: 05 *****\n",
      "Train Loss: 0.3506 | Train Acc: 79.33%\n",
      "  6%|██▌                                      | 5/80 [14:46<3:41:31, 177.23s/it]Epoch: 06 *****\n",
      "Train Loss: 0.3420 | Train Acc: 79.49%\n",
      "  8%|███                                      | 6/80 [17:44<3:38:45, 177.37s/it]Epoch: 07 *****\n",
      "Train Loss: 0.3361 | Train Acc: 79.70%\n",
      "  9%|███▌                                     | 7/80 [20:39<3:34:56, 176.67s/it]Epoch: 08 *****\n",
      "Train Loss: 0.3331 | Train Acc: 79.81%\n",
      " 10%|████                                     | 8/80 [23:35<3:31:41, 176.42s/it]Epoch: 09 *****\n",
      "Train Loss: 0.3284 | Train Acc: 79.95%\n",
      " 11%|████▌                                    | 9/80 [26:30<3:28:22, 176.09s/it]Epoch: 10 *****\n",
      "Train Loss: 0.3244 | Train Acc: 80.07%\n",
      " 12%|█████                                   | 10/80 [29:27<3:25:35, 176.22s/it]Epoch: 11 *****\n",
      "Train Loss: 0.3211 | Train Acc: 80.16%\n",
      " 14%|█████▌                                  | 11/80 [32:23<3:22:36, 176.18s/it]Epoch: 12 *****\n",
      "Train Loss: 0.3195 | Train Acc: 80.21%\n",
      " 15%|██████                                  | 12/80 [35:17<3:19:02, 175.62s/it]Epoch: 13 *****\n",
      "Train Loss: 0.3164 | Train Acc: 80.32%\n",
      " 16%|██████▌                                 | 13/80 [38:12<3:16:01, 175.54s/it]Epoch: 14 *****\n",
      "Train Loss: 0.3156 | Train Acc: 80.31%\n",
      " 18%|███████                                 | 14/80 [41:08<3:13:00, 175.47s/it]Epoch: 15 *****\n",
      "Train Loss: 0.3138 | Train Acc: 80.36%\n",
      " 19%|███████▌                                | 15/80 [44:03<3:10:09, 175.53s/it]Epoch: 16 *****\n",
      "Train Loss: 0.3131 | Train Acc: 80.35%\n",
      " 20%|████████                                | 16/80 [46:59<3:07:22, 175.67s/it]Epoch: 17 *****\n",
      "Train Loss: 0.3107 | Train Acc: 80.48%\n",
      " 21%|████████▌                               | 17/80 [49:57<3:04:59, 176.18s/it]Epoch: 18 *****\n",
      "Train Loss: 0.3096 | Train Acc: 80.48%\n",
      " 22%|█████████                               | 18/80 [52:55<3:02:32, 176.65s/it]Epoch: 19 *****\n",
      "Train Loss: 0.3086 | Train Acc: 80.50%\n",
      " 24%|█████████▌                              | 19/80 [55:51<2:59:33, 176.62s/it]Epoch: 20 *****\n",
      "Train Loss: 0.3071 | Train Acc: 80.55%\n",
      " 25%|██████████                              | 20/80 [58:48<2:56:34, 176.58s/it]Epoch: 21 *****\n",
      "Train Loss: 0.3071 | Train Acc: 80.53%\n",
      " 26%|█████████▉                            | 21/80 [1:01:44<2:53:42, 176.65s/it]Epoch: 22 *****\n",
      "Train Loss: 0.3067 | Train Acc: 80.54%\n",
      " 28%|██████████▍                           | 22/80 [1:04:40<2:50:29, 176.38s/it]Epoch: 23 *****\n",
      "Train Loss: 0.3052 | Train Acc: 80.60%\n",
      " 29%|██████████▉                           | 23/80 [1:07:36<2:47:18, 176.11s/it]Epoch: 24 *****\n",
      "Train Loss: 0.3046 | Train Acc: 80.57%\n",
      " 30%|███████████▍                          | 24/80 [1:10:31<2:44:17, 176.02s/it]Epoch: 25 *****\n",
      "Train Loss: 0.3033 | Train Acc: 80.63%\n",
      " 31%|███████████▉                          | 25/80 [1:13:26<2:41:04, 175.72s/it]Epoch: 26 *****\n",
      "Train Loss: 0.3027 | Train Acc: 80.62%\n",
      " 32%|████████████▎                         | 26/80 [1:16:25<2:39:02, 176.71s/it]Epoch: 27 *****\n",
      "Train Loss: 0.3022 | Train Acc: 80.63%\n",
      " 34%|████████████▊                         | 27/80 [1:19:22<2:35:55, 176.52s/it]Epoch: 28 *****\n",
      "Train Loss: 0.3026 | Train Acc: 80.65%\n",
      " 35%|█████████████▎                        | 28/80 [1:22:21<2:33:42, 177.36s/it]Epoch: 29 *****\n",
      "Train Loss: 0.3010 | Train Acc: 80.69%\n",
      " 36%|█████████████▊                        | 29/80 [1:25:22<2:31:39, 178.43s/it]Epoch: 30 *****\n",
      "Train Loss: 0.3009 | Train Acc: 80.68%\n",
      " 38%|██████████████▎                       | 30/80 [1:28:22<2:29:14, 179.09s/it]Epoch: 31 *****\n",
      "Train Loss: 0.2998 | Train Acc: 80.72%\n",
      " 39%|██████████████▋                       | 31/80 [1:31:17<2:25:02, 177.60s/it]Epoch: 32 *****\n",
      "Train Loss: 0.2994 | Train Acc: 80.73%\n",
      " 40%|███████████████▏                      | 32/80 [1:34:10<2:21:10, 176.47s/it]Epoch: 33 *****\n",
      "Train Loss: 0.2997 | Train Acc: 80.73%\n",
      " 41%|███████████████▋                      | 33/80 [1:37:03<2:17:22, 175.38s/it]Epoch: 34 *****\n",
      "Train Loss: 0.2987 | Train Acc: 80.72%\n",
      " 42%|████████████████▏                     | 34/80 [1:39:55<2:13:43, 174.43s/it]Epoch: 35 *****\n",
      "Train Loss: 0.2989 | Train Acc: 80.74%\n",
      " 44%|████████████████▋                     | 35/80 [1:42:48<2:10:18, 173.73s/it]Epoch: 36 *****\n",
      "Train Loss: 0.2978 | Train Acc: 80.77%\n",
      " 45%|█████████████████                     | 36/80 [1:45:40<2:07:05, 173.31s/it]Epoch: 37 *****\n",
      "Train Loss: 0.2979 | Train Acc: 80.75%\n",
      " 46%|█████████████████▌                    | 37/80 [1:48:31<2:03:46, 172.70s/it]Epoch: 38 *****\n",
      "Train Loss: 0.2979 | Train Acc: 80.77%\n",
      " 48%|██████████████████                    | 38/80 [1:51:22<2:00:34, 172.26s/it]Epoch: 39 *****\n",
      "Train Loss: 0.2973 | Train Acc: 80.78%\n",
      " 49%|██████████████████▌                   | 39/80 [1:54:14<1:57:39, 172.18s/it]Epoch: 40 *****\n",
      "Train Loss: 0.2969 | Train Acc: 80.80%\n",
      " 50%|███████████████████                   | 40/80 [1:57:07<1:54:56, 172.40s/it]Epoch: 41 *****\n",
      "Train Loss: 0.2972 | Train Acc: 80.77%\n",
      " 51%|███████████████████▍                  | 41/80 [2:00:00<1:52:02, 172.36s/it]Epoch: 42 *****\n",
      "Train Loss: 0.2964 | Train Acc: 80.80%\n",
      " 52%|███████████████████▉                  | 42/80 [2:02:52<1:49:05, 172.25s/it]Epoch: 43 *****\n",
      "Train Loss: 0.2968 | Train Acc: 80.78%\n",
      " 54%|████████████████████▍                 | 43/80 [2:05:44<1:46:14, 172.28s/it]Epoch: 44 *****\n",
      "Train Loss: 0.2958 | Train Acc: 80.80%\n",
      " 55%|████████████████████▉                 | 44/80 [2:08:35<1:43:11, 171.98s/it]Epoch: 45 *****\n",
      "Train Loss: 0.2960 | Train Acc: 80.82%\n",
      " 56%|█████████████████████▍                | 45/80 [2:11:27<1:40:16, 171.90s/it]Epoch: 46 *****\n",
      "Train Loss: 0.2948 | Train Acc: 80.85%\n",
      " 57%|█████████████████████▊                | 46/80 [2:14:18<1:37:19, 171.75s/it]Epoch: 47 *****\n",
      "Train Loss: 0.2953 | Train Acc: 80.82%\n",
      " 59%|██████████████████████▎               | 47/80 [2:17:10<1:34:30, 171.84s/it]Epoch: 48 *****\n",
      "Train Loss: 0.2948 | Train Acc: 80.84%\n",
      " 60%|██████████████████████▊               | 48/80 [2:20:02<1:31:41, 171.92s/it]Epoch: 49 *****\n",
      "Train Loss: 0.2943 | Train Acc: 80.84%\n",
      " 61%|███████████████████████▎              | 49/80 [2:22:54<1:28:45, 171.78s/it]Epoch: 50 *****\n",
      "Train Loss: 0.2951 | Train Acc: 80.84%\n",
      " 62%|███████████████████████▊              | 50/80 [2:25:46<1:25:56, 171.89s/it]Epoch: 51 *****\n",
      "Train Loss: 0.2941 | Train Acc: 80.84%\n",
      "Early stopping. No improvement in 51 epochs.\n",
      " 62%|███████████████████████▊              | 50/80 [2:28:39<1:29:11, 178.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# amnet_train\n",
    "!python Atom_matching_network/train/amnet_train_fold.py --fold 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9de718dc-f5b2-4b72-80bf-3c6e66058f42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "Memory allocated: 0.00 GB\n",
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "FMNet(\n",
      "gnn=GIN(111, 512, num_layers=3, batch_norm=False, cat=True, lin=True),\n",
      "\n",
      "Namespace(num_wl_iterations=3, node_features_dim=111, edge_feature_dim=10, santitize=False, embedding_dim=512, num_layers=3, lr=0.0001, n_epochs=80, batch_size=1, fold=2)\n",
      "  0%|                                                    | 0/80 [00:00<?, ?it/s]Epoch: 01 *****\n",
      "Train Loss: 0.5275 | Train Acc: 74.42%\n",
      "  1%|▌                                        | 1/80 [02:57<3:53:56, 177.68s/it]Epoch: 02 *****\n",
      "Train Loss: 0.4003 | Train Acc: 77.90%\n",
      "  2%|█                                        | 2/80 [05:55<3:51:01, 177.71s/it]Epoch: 03 *****\n",
      "Train Loss: 0.3713 | Train Acc: 78.74%\n",
      "  4%|█▌                                       | 3/80 [08:54<3:49:08, 178.56s/it]Epoch: 04 *****\n",
      "Train Loss: 0.3589 | Train Acc: 79.14%\n",
      "  5%|██                                       | 4/80 [11:57<3:48:18, 180.25s/it]Epoch: 05 *****\n",
      "Train Loss: 0.3467 | Train Acc: 79.42%\n",
      "  6%|██▌                                      | 5/80 [15:00<3:46:28, 181.18s/it]Epoch: 06 *****\n",
      "Train Loss: 0.3429 | Train Acc: 79.66%\n",
      "  8%|███                                      | 6/80 [18:03<3:44:09, 181.75s/it]Epoch: 07 *****\n",
      "Train Loss: 0.3332 | Train Acc: 79.83%\n",
      "  9%|███▌                                     | 7/80 [21:06<3:41:29, 182.05s/it]Epoch: 08 *****\n",
      "Train Loss: 0.3302 | Train Acc: 79.96%\n",
      " 10%|████                                     | 8/80 [24:09<3:38:45, 182.30s/it]Epoch: 09 *****\n",
      "Train Loss: 0.3259 | Train Acc: 80.11%\n",
      " 11%|████▌                                    | 9/80 [27:12<3:35:59, 182.53s/it]Epoch: 10 *****\n",
      "Train Loss: 0.3231 | Train Acc: 80.14%\n",
      " 12%|█████                                   | 10/80 [30:13<3:32:42, 182.33s/it]Epoch: 11 *****\n",
      "Train Loss: 0.3192 | Train Acc: 80.24%\n",
      " 14%|█████▌                                  | 11/80 [33:15<3:29:23, 182.08s/it]Epoch: 12 *****\n",
      "Train Loss: 0.3171 | Train Acc: 80.32%\n",
      " 15%|██████                                  | 12/80 [36:16<3:26:09, 181.90s/it]Epoch: 13 *****\n",
      "Train Loss: 0.3140 | Train Acc: 80.39%\n",
      " 16%|██████▌                                 | 13/80 [39:20<3:23:50, 182.54s/it]Epoch: 14 *****\n",
      "Train Loss: 0.3130 | Train Acc: 80.43%\n",
      " 18%|███████                                 | 14/80 [42:24<3:21:02, 182.76s/it]Epoch: 15 *****\n",
      "Train Loss: 0.3105 | Train Acc: 80.50%\n",
      " 19%|███████▌                                | 15/80 [45:27<3:18:08, 182.90s/it]Epoch: 16 *****\n",
      "Train Loss: 0.3094 | Train Acc: 80.55%\n",
      " 20%|████████                                | 16/80 [48:29<3:14:50, 182.67s/it]Epoch: 17 *****\n",
      "Train Loss: 0.3084 | Train Acc: 80.56%\n",
      " 21%|████████▌                               | 17/80 [51:33<3:12:03, 182.91s/it]Epoch: 18 *****\n",
      "Train Loss: 0.3070 | Train Acc: 80.60%\n",
      " 22%|█████████                               | 18/80 [54:34<3:08:31, 182.45s/it]Epoch: 19 *****\n",
      "Train Loss: 0.3051 | Train Acc: 80.66%\n",
      " 24%|█████████▌                              | 19/80 [58:53<3:28:46, 205.36s/it]Epoch: 20 *****\n",
      "Train Loss: 0.3054 | Train Acc: 80.65%\n",
      " 25%|█████████▌                            | 20/80 [1:05:14<4:18:04, 258.08s/it]Epoch: 21 *****\n",
      "Train Loss: 0.3046 | Train Acc: 80.66%\n",
      " 26%|█████████▉                            | 21/80 [1:11:25<4:47:08, 292.00s/it]Epoch: 22 *****\n",
      "Train Loss: 0.3035 | Train Acc: 80.72%\n",
      " 28%|██████████▍                           | 22/80 [1:17:48<5:08:44, 319.38s/it]Epoch: 23 *****\n",
      "Train Loss: 0.3020 | Train Acc: 80.70%\n",
      " 29%|██████████▉                           | 23/80 [1:24:02<5:19:00, 335.79s/it]Epoch: 24 *****\n",
      "Train Loss: 0.3011 | Train Acc: 80.73%\n",
      " 30%|███████████▍                          | 24/80 [1:30:21<5:25:30, 348.75s/it]Epoch: 25 *****\n",
      "Train Loss: 0.3000 | Train Acc: 80.77%\n",
      " 31%|███████████▉                          | 25/80 [1:34:24<4:50:42, 317.14s/it]Epoch: 26 *****\n",
      "Train Loss: 0.2999 | Train Acc: 80.79%\n",
      " 32%|████████████▎                         | 26/80 [1:37:22<4:07:49, 275.37s/it]Epoch: 27 *****\n",
      "Train Loss: 0.2997 | Train Acc: 80.79%\n",
      " 34%|████████████▊                         | 27/80 [1:40:23<3:38:03, 246.85s/it]Epoch: 28 *****\n",
      "Train Loss: 0.2990 | Train Acc: 80.78%\n",
      " 35%|█████████████▎                        | 28/80 [1:43:27<3:17:34, 227.98s/it]Epoch: 29 *****\n",
      "Train Loss: 0.2989 | Train Acc: 80.79%\n",
      " 36%|█████████████▊                        | 29/80 [1:46:29<3:02:11, 214.35s/it]Epoch: 30 *****\n",
      "Train Loss: 0.2990 | Train Acc: 80.79%\n",
      " 38%|██████████████▎                       | 30/80 [1:49:31<2:50:27, 204.55s/it]Epoch: 31 *****\n",
      "Train Loss: 0.2970 | Train Acc: 80.82%\n",
      " 39%|██████████████▋                       | 31/80 [1:52:30<2:40:50, 196.95s/it]Epoch: 32 *****\n",
      "Train Loss: 0.2963 | Train Acc: 80.85%\n",
      " 40%|███████████████▏                      | 32/80 [1:55:29<2:33:19, 191.65s/it]Epoch: 33 *****\n",
      "Train Loss: 0.2971 | Train Acc: 80.84%\n",
      " 41%|███████████████▋                      | 33/80 [1:58:34<2:28:32, 189.62s/it]Epoch: 34 *****\n",
      "Train Loss: 0.2965 | Train Acc: 80.88%\n",
      " 42%|████████████████▏                     | 34/80 [2:01:38<2:23:57, 187.77s/it]Epoch: 35 *****\n",
      "Train Loss: 0.2966 | Train Acc: 80.88%\n",
      " 44%|████████████████▋                     | 35/80 [2:04:40<2:19:34, 186.10s/it]Epoch: 36 *****\n",
      "Train Loss: 0.2959 | Train Acc: 80.86%\n",
      " 45%|█████████████████                     | 36/80 [2:07:42<2:15:35, 184.91s/it]Epoch: 37 *****\n",
      "Train Loss: 0.2950 | Train Acc: 80.89%\n",
      " 46%|█████████████████▌                    | 37/80 [2:10:42<2:11:33, 183.57s/it]Epoch: 38 *****\n",
      "Train Loss: 0.2947 | Train Acc: 80.90%\n",
      " 48%|██████████████████                    | 38/80 [2:13:47<2:08:38, 183.76s/it]Epoch: 39 *****\n",
      "Train Loss: 0.2941 | Train Acc: 80.91%\n",
      " 49%|██████████████████▌                   | 39/80 [2:16:54<2:06:21, 184.90s/it]Epoch: 40 *****\n",
      "Train Loss: 0.2944 | Train Acc: 80.93%\n",
      " 50%|███████████████████                   | 40/80 [2:19:58<2:03:04, 184.61s/it]Epoch: 41 *****\n",
      "Train Loss: 0.2938 | Train Acc: 80.95%\n",
      " 51%|███████████████████▍                  | 41/80 [2:23:01<1:59:41, 184.14s/it]Epoch: 42 *****\n",
      "Train Loss: 0.2941 | Train Acc: 80.92%\n",
      " 52%|███████████████████▉                  | 42/80 [2:26:08<1:57:03, 184.83s/it]Epoch: 43 *****\n",
      "Train Loss: 0.2935 | Train Acc: 80.94%\n",
      " 54%|████████████████████▍                 | 43/80 [2:29:16<1:54:42, 186.01s/it]Epoch: 44 *****\n",
      "Train Loss: 0.2934 | Train Acc: 80.94%\n",
      " 55%|████████████████████▉                 | 44/80 [2:32:18<1:50:52, 184.79s/it]Epoch: 45 *****\n",
      "Train Loss: 0.2934 | Train Acc: 80.95%\n",
      " 56%|█████████████████████▍                | 45/80 [2:35:20<1:47:18, 183.96s/it]Epoch: 46 *****\n",
      "Train Loss: 0.2930 | Train Acc: 80.97%\n",
      " 57%|█████████████████████▊                | 46/80 [2:38:22<1:43:55, 183.38s/it]Epoch: 47 *****\n",
      "Train Loss: 0.2939 | Train Acc: 80.93%\n",
      " 59%|██████████████████████▎               | 47/80 [2:41:25<1:40:46, 183.24s/it]Epoch: 48 *****\n",
      "Train Loss: 0.2922 | Train Acc: 80.97%\n",
      "Early stopping. No improvement in 48 epochs.\n",
      " 59%|██████████████████████▎               | 47/80 [2:44:26<1:55:27, 209.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# amnet_train\n",
    "!python Atom_matching_network/train/amnet_train_fold.py --fold 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0afdbaee-91b0-43ad-9d03-96d324fd7447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "Memory allocated: 0.00 GB\n",
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "FMNet(\n",
      "gnn=GIN(111, 512, num_layers=3, batch_norm=False, cat=True, lin=True),\n",
      "\n",
      "Namespace(num_wl_iterations=3, node_features_dim=111, edge_feature_dim=10, santitize=False, embedding_dim=512, num_layers=3, lr=0.0001, n_epochs=80, batch_size=1, fold=3)\n",
      "  0%|                                                    | 0/80 [00:00<?, ?it/s]Epoch: 01 *****\n",
      "Train Loss: 0.5281 | Train Acc: 74.35%\n",
      "  1%|▌                                        | 1/80 [03:01<3:58:59, 181.51s/it]Epoch: 02 *****\n",
      "Train Loss: 0.4037 | Train Acc: 77.77%\n",
      "  2%|█                                        | 2/80 [06:02<3:55:18, 181.01s/it]Epoch: 03 *****\n",
      "Train Loss: 0.3781 | Train Acc: 78.49%\n",
      "  4%|█▌                                       | 3/80 [09:04<3:53:09, 181.68s/it]Epoch: 04 *****\n",
      "Train Loss: 0.3608 | Train Acc: 79.04%\n",
      "  5%|██                                       | 4/80 [12:01<3:47:48, 179.85s/it]Epoch: 05 *****\n",
      "Train Loss: 0.3497 | Train Acc: 79.38%\n",
      "  6%|██▌                                      | 5/80 [14:59<3:43:56, 179.16s/it]Epoch: 06 *****\n",
      "Train Loss: 0.3418 | Train Acc: 79.61%\n",
      "  8%|███                                      | 6/80 [17:57<3:40:24, 178.71s/it]Epoch: 07 *****\n",
      "Train Loss: 0.3363 | Train Acc: 79.71%\n",
      "  9%|███▌                                     | 7/80 [20:56<3:37:35, 178.84s/it]Epoch: 08 *****\n",
      "Train Loss: 0.3309 | Train Acc: 79.87%\n",
      " 10%|████                                     | 8/80 [23:55<3:34:34, 178.81s/it]Epoch: 09 *****\n",
      "Train Loss: 0.3274 | Train Acc: 79.98%\n",
      " 11%|████▌                                    | 9/80 [26:53<3:31:21, 178.61s/it]Epoch: 10 *****\n",
      "Train Loss: 0.3251 | Train Acc: 80.02%\n",
      " 12%|█████                                   | 10/80 [29:52<3:28:22, 178.60s/it]Epoch: 11 *****\n",
      "Train Loss: 0.3221 | Train Acc: 80.12%\n",
      " 14%|█████▌                                  | 11/80 [32:47<3:24:23, 177.73s/it]Epoch: 12 *****\n",
      "Train Loss: 0.3188 | Train Acc: 80.19%\n",
      " 15%|██████                                  | 12/80 [35:48<3:22:35, 178.76s/it]Epoch: 13 *****\n",
      "Train Loss: 0.3164 | Train Acc: 80.26%\n",
      " 16%|██████▌                                 | 13/80 [38:49<3:20:04, 179.17s/it]Epoch: 14 *****\n",
      "Train Loss: 0.3159 | Train Acc: 80.30%\n",
      " 18%|███████                                 | 14/80 [41:53<3:18:47, 180.72s/it]Epoch: 15 *****\n",
      "Train Loss: 0.3134 | Train Acc: 80.38%\n",
      " 19%|███████▌                                | 15/80 [44:52<3:15:22, 180.35s/it]Epoch: 16 *****\n",
      "Train Loss: 0.3111 | Train Acc: 80.40%\n",
      " 20%|████████                                | 16/80 [47:50<3:11:26, 179.48s/it]Epoch: 17 *****\n",
      "Train Loss: 0.3100 | Train Acc: 80.48%\n",
      " 21%|████████▌                               | 17/80 [50:44<3:06:55, 178.03s/it]Epoch: 18 *****\n",
      "Train Loss: 0.3084 | Train Acc: 80.47%\n",
      " 22%|█████████                               | 18/80 [53:39<3:02:53, 176.99s/it]Epoch: 19 *****\n",
      "Train Loss: 0.3082 | Train Acc: 80.47%\n",
      " 24%|█████████▌                              | 19/80 [56:31<2:58:32, 175.61s/it]Epoch: 20 *****\n",
      "Train Loss: 0.3076 | Train Acc: 80.51%\n",
      " 25%|██████████                              | 20/80 [59:31<2:56:42, 176.71s/it]Epoch: 21 *****\n",
      "Train Loss: 0.3059 | Train Acc: 80.53%\n",
      " 26%|█████████▉                            | 21/80 [1:02:26<2:53:19, 176.25s/it]Epoch: 22 *****\n",
      "Train Loss: 0.3055 | Train Acc: 80.56%\n",
      " 28%|██████████▍                           | 22/80 [1:05:20<2:49:45, 175.61s/it]Epoch: 23 *****\n",
      "Train Loss: 0.3041 | Train Acc: 80.60%\n",
      " 29%|██████████▉                           | 23/80 [1:08:13<2:46:13, 174.98s/it]Epoch: 24 *****\n",
      "Train Loss: 0.3049 | Train Acc: 80.66%\n",
      " 30%|███████████▍                          | 24/80 [1:11:08<2:43:13, 174.89s/it]Epoch: 25 *****\n",
      "Train Loss: 0.3033 | Train Acc: 80.66%\n",
      " 31%|███████████▉                          | 25/80 [1:14:02<2:40:06, 174.67s/it]Epoch: 26 *****\n",
      "Train Loss: 0.3022 | Train Acc: 80.67%\n",
      " 32%|████████████▎                         | 26/80 [1:16:57<2:37:06, 174.56s/it]Epoch: 27 *****\n",
      "Train Loss: 0.3009 | Train Acc: 80.71%\n",
      " 34%|████████████▊                         | 27/80 [1:19:51<2:34:02, 174.39s/it]Epoch: 28 *****\n",
      "Train Loss: 0.3011 | Train Acc: 80.71%\n",
      " 35%|█████████████▎                        | 28/80 [1:22:45<2:31:00, 174.25s/it]Epoch: 29 *****\n",
      "Train Loss: 0.3005 | Train Acc: 80.69%\n",
      " 36%|█████████████▊                        | 29/80 [1:25:37<2:27:42, 173.77s/it]Epoch: 30 *****\n",
      "Train Loss: 0.3000 | Train Acc: 80.72%\n",
      " 38%|██████████████▎                       | 30/80 [1:28:30<2:24:34, 173.50s/it]Epoch: 31 *****\n",
      "Train Loss: 0.2998 | Train Acc: 80.75%\n",
      " 39%|██████████████▋                       | 31/80 [1:31:25<2:21:57, 173.82s/it]Epoch: 32 *****\n",
      "Train Loss: 0.2994 | Train Acc: 80.74%\n",
      " 40%|███████████████▏                      | 32/80 [1:34:19<2:19:04, 173.85s/it]Epoch: 33 *****\n",
      "Train Loss: 0.2998 | Train Acc: 80.72%\n",
      " 41%|███████████████▋                      | 33/80 [1:37:14<2:16:28, 174.22s/it]Epoch: 34 *****\n",
      "Train Loss: 0.2987 | Train Acc: 80.73%\n",
      " 42%|████████████████▏                     | 34/80 [1:40:08<2:13:40, 174.35s/it]Epoch: 35 *****\n",
      "Train Loss: 0.2979 | Train Acc: 80.77%\n",
      " 44%|████████████████▋                     | 35/80 [1:43:01<2:10:29, 174.00s/it]Epoch: 36 *****\n",
      "Train Loss: 0.2964 | Train Acc: 80.83%\n",
      " 45%|█████████████████                     | 36/80 [1:45:55<2:07:28, 173.82s/it]Epoch: 37 *****\n",
      "Train Loss: 0.2979 | Train Acc: 80.76%\n",
      " 46%|█████████████████▌                    | 37/80 [1:48:49<2:04:43, 174.03s/it]Epoch: 38 *****\n",
      "Train Loss: 0.2965 | Train Acc: 80.81%\n",
      " 48%|██████████████████                    | 38/80 [1:51:42<2:01:34, 173.68s/it]Epoch: 39 *****\n",
      "Train Loss: 0.2966 | Train Acc: 80.79%\n",
      " 49%|██████████████████▌                   | 39/80 [1:54:35<1:58:30, 173.42s/it]Epoch: 40 *****\n",
      "Train Loss: 0.2955 | Train Acc: 80.84%\n",
      " 50%|███████████████████                   | 40/80 [1:57:28<1:55:28, 173.20s/it]Epoch: 41 *****\n",
      "Train Loss: 0.2957 | Train Acc: 80.82%\n",
      " 51%|███████████████████▍                  | 41/80 [2:00:20<1:52:29, 173.05s/it]Epoch: 42 *****\n",
      "Train Loss: 0.2963 | Train Acc: 80.82%\n",
      " 52%|███████████████████▉                  | 42/80 [2:03:16<1:50:02, 173.74s/it]Epoch: 43 *****\n",
      "Train Loss: 0.2957 | Train Acc: 80.81%\n",
      " 54%|████████████████████▍                 | 43/80 [2:06:10<1:47:09, 173.77s/it]Epoch: 44 *****\n",
      "Train Loss: 0.2943 | Train Acc: 80.85%\n",
      " 55%|████████████████████▉                 | 44/80 [2:09:04<1:44:26, 174.06s/it]Epoch: 45 *****\n",
      "Train Loss: 0.2943 | Train Acc: 80.87%\n",
      " 56%|█████████████████████▍                | 45/80 [2:12:00<1:41:43, 174.39s/it]Epoch: 46 *****\n",
      "Train Loss: 0.2950 | Train Acc: 80.83%\n",
      " 57%|█████████████████████▊                | 46/80 [2:14:54<1:38:48, 174.35s/it]Epoch: 47 *****\n",
      "Train Loss: 0.2944 | Train Acc: 80.83%\n",
      " 59%|██████████████████████▎               | 47/80 [2:17:50<1:36:08, 174.80s/it]Epoch: 48 *****\n",
      "Train Loss: 0.2941 | Train Acc: 80.91%\n",
      " 60%|██████████████████████▊               | 48/80 [2:20:47<1:33:33, 175.42s/it]Epoch: 49 *****\n",
      "Train Loss: 0.2942 | Train Acc: 80.85%\n",
      " 61%|███████████████████████▎              | 49/80 [2:23:48<1:31:38, 177.37s/it]Epoch: 50 *****\n",
      "Train Loss: 0.2922 | Train Acc: 80.91%\n",
      " 62%|███████████████████████▊              | 50/80 [2:26:47<1:28:48, 177.63s/it]Epoch: 51 *****\n",
      "Train Loss: 0.2944 | Train Acc: 80.88%\n",
      " 64%|████████████████████████▏             | 51/80 [2:29:43<1:25:39, 177.23s/it]Epoch: 52 *****\n",
      "Train Loss: 0.2928 | Train Acc: 80.86%\n",
      " 65%|████████████████████████▋             | 52/80 [2:32:41<1:22:46, 177.39s/it]Epoch: 53 *****\n",
      "Train Loss: 0.2926 | Train Acc: 80.91%\n",
      " 66%|█████████████████████████▏            | 53/80 [2:35:39<1:19:53, 177.53s/it]Epoch: 54 *****\n",
      "Train Loss: 0.2929 | Train Acc: 80.88%\n",
      " 68%|█████████████████████████▋            | 54/80 [2:38:40<1:17:22, 178.55s/it]Epoch: 55 *****\n",
      "Train Loss: 0.2932 | Train Acc: 80.89%\n",
      " 69%|██████████████████████████▏           | 55/80 [2:41:38<1:14:22, 178.48s/it]Epoch: 56 *****\n",
      "Train Loss: 0.2931 | Train Acc: 80.94%\n",
      " 70%|██████████████████████████▌           | 56/80 [2:44:32<1:10:52, 177.19s/it]Epoch: 57 *****\n",
      "Train Loss: 0.2928 | Train Acc: 80.90%\n",
      " 71%|███████████████████████████           | 57/80 [2:47:26<1:07:31, 176.16s/it]Epoch: 58 *****\n",
      "Train Loss: 0.2915 | Train Acc: 80.93%\n",
      " 72%|███████████████████████████▌          | 58/80 [2:50:19<1:04:19, 175.41s/it]Epoch: 59 *****\n",
      "Train Loss: 0.2922 | Train Acc: 80.88%\n",
      " 74%|████████████████████████████          | 59/80 [2:53:16<1:01:30, 175.73s/it]Epoch: 60 *****\n",
      "Train Loss: 0.2914 | Train Acc: 80.94%\n",
      " 75%|██████████████████████████████          | 60/80 [2:56:15<58:52, 176.64s/it]Epoch: 61 *****\n",
      "Train Loss: 0.2920 | Train Acc: 80.89%\n",
      " 76%|██████████████████████████████▌         | 61/80 [2:59:12<55:59, 176.82s/it]Epoch: 62 *****\n",
      "Train Loss: 0.2913 | Train Acc: 80.92%\n",
      " 78%|███████████████████████████████         | 62/80 [3:02:09<53:02, 176.80s/it]Epoch: 63 *****\n",
      "Train Loss: 0.2911 | Train Acc: 80.92%\n",
      " 79%|███████████████████████████████▌        | 63/80 [3:05:05<50:01, 176.54s/it]Epoch: 64 *****\n",
      "Train Loss: 0.2904 | Train Acc: 80.97%\n",
      " 80%|████████████████████████████████        | 64/80 [3:08:04<47:17, 177.32s/it]Epoch: 65 *****\n",
      "Train Loss: 0.2918 | Train Acc: 80.92%\n",
      " 81%|████████████████████████████████▌       | 65/80 [3:11:02<44:24, 177.64s/it]Epoch: 66 *****\n",
      "Train Loss: 0.2910 | Train Acc: 80.93%\n",
      " 82%|█████████████████████████████████       | 66/80 [3:14:00<41:27, 177.70s/it]Epoch: 67 *****\n",
      "Train Loss: 0.2906 | Train Acc: 80.98%\n",
      " 84%|█████████████████████████████████▌      | 67/80 [3:16:59<38:33, 177.99s/it]Epoch: 68 *****\n",
      "Train Loss: 0.2902 | Train Acc: 80.96%\n",
      " 85%|██████████████████████████████████      | 68/80 [3:19:53<35:21, 176.78s/it]Epoch: 69 *****\n",
      "Train Loss: 0.2911 | Train Acc: 80.93%\n",
      " 86%|██████████████████████████████████▌     | 69/80 [3:22:47<32:17, 176.09s/it]Epoch: 70 *****\n",
      "Train Loss: 0.2901 | Train Acc: 80.99%\n",
      "Early stopping. No improvement in 70 epochs.\n",
      " 86%|██████████████████████████████████▌     | 69/80 [3:25:40<32:47, 178.85s/it]\n"
     ]
    }
   ],
   "source": [
    "# amnet_train\n",
    "!python Atom_matching_network/train/amnet_train_fold.py --fold 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ca8bef0-8474-424d-b608-91d51434256c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "Memory allocated: 0.00 GB\n",
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "FMNet(\n",
      "gnn=GIN(111, 512, num_layers=3, batch_norm=False, cat=True, lin=True),\n",
      "\n",
      "Namespace(num_wl_iterations=3, node_features_dim=111, edge_feature_dim=10, santitize=False, embedding_dim=512, num_layers=3, lr=0.0001, n_epochs=80, batch_size=1, fold=4)\n",
      "  0%|                                                    | 0/80 [00:00<?, ?it/s]Epoch: 01 *****\n",
      "Train Loss: 0.5269 | Train Acc: 74.45%\n",
      "  1%|▌                                        | 1/80 [02:54<3:49:46, 174.51s/it]Epoch: 02 *****\n",
      "Train Loss: 0.4064 | Train Acc: 77.70%\n",
      "  2%|█                                        | 2/80 [05:50<3:48:10, 175.52s/it]Epoch: 03 *****\n",
      "Train Loss: 0.3752 | Train Acc: 78.65%\n",
      "  4%|█▌                                       | 3/80 [08:54<3:49:53, 179.13s/it]Epoch: 04 *****\n",
      "Train Loss: 0.3584 | Train Acc: 79.06%\n",
      "  5%|██                                       | 4/80 [11:58<3:49:23, 181.09s/it]Epoch: 05 *****\n",
      "Train Loss: 0.3497 | Train Acc: 79.31%\n",
      "  6%|██▌                                      | 5/80 [15:01<3:47:23, 181.91s/it]Epoch: 06 *****\n",
      "Train Loss: 0.3421 | Train Acc: 79.57%\n",
      "  8%|███                                      | 6/80 [17:56<3:41:26, 179.55s/it]Epoch: 07 *****\n",
      "Train Loss: 0.3354 | Train Acc: 79.78%\n",
      "  9%|███▌                                     | 7/80 [20:55<3:38:10, 179.33s/it]Epoch: 08 *****\n",
      "Train Loss: 0.3303 | Train Acc: 79.93%\n",
      " 10%|████                                     | 8/80 [23:59<3:37:05, 180.91s/it]Epoch: 09 *****\n",
      "Train Loss: 0.3258 | Train Acc: 80.07%\n",
      " 11%|████▌                                    | 9/80 [27:01<3:34:16, 181.07s/it]Epoch: 10 *****\n",
      "Train Loss: 0.3228 | Train Acc: 80.11%\n",
      " 12%|█████                                   | 10/80 [29:55<3:28:41, 178.87s/it]Epoch: 11 *****\n",
      "Train Loss: 0.3196 | Train Acc: 80.17%\n",
      " 14%|█████▌                                  | 11/80 [32:49<3:24:04, 177.45s/it]Epoch: 12 *****\n",
      "Train Loss: 0.3167 | Train Acc: 80.28%\n",
      " 15%|██████                                  | 12/80 [35:43<3:20:01, 176.49s/it]Epoch: 13 *****\n",
      "Train Loss: 0.3147 | Train Acc: 80.30%\n",
      " 16%|██████▌                                 | 13/80 [38:37<3:16:07, 175.63s/it]Epoch: 14 *****\n",
      "Train Loss: 0.3128 | Train Acc: 80.42%\n",
      " 18%|███████                                 | 14/80 [41:31<3:12:42, 175.18s/it]Epoch: 15 *****\n",
      "Train Loss: 0.3117 | Train Acc: 80.43%\n",
      " 19%|███████▌                                | 15/80 [44:25<3:09:26, 174.86s/it]Epoch: 16 *****\n",
      "Train Loss: 0.3096 | Train Acc: 80.47%\n",
      " 20%|████████                                | 16/80 [47:19<3:06:08, 174.51s/it]Epoch: 17 *****\n",
      "Train Loss: 0.3091 | Train Acc: 80.50%\n",
      " 21%|████████▌                               | 17/80 [50:12<3:02:41, 173.99s/it]Epoch: 18 *****\n",
      "Train Loss: 0.3069 | Train Acc: 80.57%\n",
      " 22%|█████████                               | 18/80 [53:05<2:59:42, 173.92s/it]Epoch: 19 *****\n",
      "Train Loss: 0.3057 | Train Acc: 80.55%\n",
      " 24%|█████████▌                              | 19/80 [55:59<2:56:41, 173.79s/it]Epoch: 20 *****\n",
      "Train Loss: 0.3057 | Train Acc: 80.58%\n",
      " 25%|██████████                              | 20/80 [58:53<2:53:49, 173.82s/it]Epoch: 21 *****\n",
      "Train Loss: 0.3048 | Train Acc: 80.63%\n",
      " 26%|█████████▉                            | 21/80 [1:01:45<2:50:37, 173.51s/it]Epoch: 22 *****\n",
      "Train Loss: 0.3033 | Train Acc: 80.65%\n",
      " 28%|██████████▍                           | 22/80 [1:04:40<2:48:00, 173.79s/it]Epoch: 23 *****\n",
      "Train Loss: 0.3024 | Train Acc: 80.70%\n",
      " 29%|██████████▉                           | 23/80 [1:07:34<2:45:05, 173.78s/it]Epoch: 24 *****\n",
      "Train Loss: 0.3012 | Train Acc: 80.71%\n",
      " 30%|███████████▍                          | 24/80 [1:10:33<2:43:38, 175.33s/it]Epoch: 25 *****\n",
      "Train Loss: 0.3013 | Train Acc: 80.74%\n",
      " 31%|███████████▉                          | 25/80 [1:13:55<2:48:17, 183.58s/it]Epoch: 26 *****\n",
      "Train Loss: 0.3004 | Train Acc: 80.74%\n",
      " 32%|████████████▎                         | 26/80 [1:16:54<2:43:54, 182.11s/it]Epoch: 27 *****\n",
      "Train Loss: 0.2994 | Train Acc: 80.77%\n",
      " 34%|████████████▊                         | 27/80 [1:19:55<2:40:37, 181.83s/it]Epoch: 28 *****\n",
      "Train Loss: 0.2993 | Train Acc: 80.73%\n",
      " 35%|█████████████▎                        | 28/80 [1:22:59<2:37:58, 182.28s/it]Epoch: 29 *****\n",
      "Train Loss: 0.2988 | Train Acc: 80.76%\n",
      " 36%|█████████████▊                        | 29/80 [1:26:02<2:35:19, 182.74s/it]Epoch: 30 *****\n",
      "Train Loss: 0.2976 | Train Acc: 80.78%\n",
      " 38%|██████████████▎                       | 30/80 [1:29:06<2:32:24, 182.88s/it]Epoch: 31 *****\n",
      "Train Loss: 0.2981 | Train Acc: 80.79%\n",
      " 39%|██████████████▋                       | 31/80 [1:32:10<2:29:36, 183.20s/it]Epoch: 32 *****\n",
      "Train Loss: 0.2970 | Train Acc: 80.83%\n",
      " 40%|███████████████▏                      | 32/80 [1:35:14<2:26:47, 183.49s/it]Epoch: 33 *****\n",
      "Train Loss: 0.2971 | Train Acc: 80.81%\n",
      " 41%|███████████████▋                      | 33/80 [1:38:17<2:23:44, 183.49s/it]Epoch: 34 *****\n",
      "Train Loss: 0.2958 | Train Acc: 80.82%\n",
      " 42%|████████████████▏                     | 34/80 [1:41:22<2:20:58, 183.89s/it]Epoch: 35 *****\n",
      "Train Loss: 0.2961 | Train Acc: 80.84%\n",
      " 44%|████████████████▋                     | 35/80 [1:44:28<2:18:15, 184.35s/it]Epoch: 36 *****\n",
      "Train Loss: 0.2953 | Train Acc: 80.87%\n",
      " 45%|█████████████████                     | 36/80 [1:47:32<2:15:13, 184.39s/it]Epoch: 37 *****\n",
      "Train Loss: 0.2948 | Train Acc: 80.86%\n",
      " 46%|█████████████████▌                    | 37/80 [1:50:33<2:11:25, 183.39s/it]Epoch: 38 *****\n",
      "Train Loss: 0.2944 | Train Acc: 80.91%\n",
      " 48%|██████████████████                    | 38/80 [1:53:27<2:06:18, 180.44s/it]Epoch: 39 *****\n",
      "Train Loss: 0.2938 | Train Acc: 80.89%\n",
      " 49%|██████████████████▌                   | 39/80 [1:56:20<2:01:54, 178.41s/it]Epoch: 40 *****\n",
      "Train Loss: 0.2942 | Train Acc: 80.87%\n",
      " 50%|███████████████████                   | 40/80 [1:59:14<1:58:03, 177.08s/it]Epoch: 41 *****\n",
      "Train Loss: 0.2937 | Train Acc: 80.89%\n",
      " 51%|███████████████████▍                  | 41/80 [2:02:09<1:54:35, 176.31s/it]Epoch: 42 *****\n",
      "Train Loss: 0.2942 | Train Acc: 80.88%\n",
      " 52%|███████████████████▉                  | 42/80 [2:05:03<1:51:13, 175.63s/it]Epoch: 43 *****\n",
      "Train Loss: 0.2941 | Train Acc: 80.89%\n",
      " 54%|████████████████████▍                 | 43/80 [2:07:56<1:47:56, 175.03s/it]Epoch: 44 *****\n",
      "Train Loss: 0.2926 | Train Acc: 80.92%\n",
      " 55%|████████████████████▉                 | 44/80 [2:10:51<1:44:54, 174.86s/it]Epoch: 45 *****\n",
      "Train Loss: 0.2934 | Train Acc: 80.89%\n",
      " 56%|█████████████████████▍                | 45/80 [2:13:45<1:41:52, 174.64s/it]Epoch: 46 *****\n",
      "Train Loss: 0.2929 | Train Acc: 80.88%\n",
      " 57%|█████████████████████▊                | 46/80 [2:16:39<1:38:48, 174.38s/it]Epoch: 47 *****\n",
      "Train Loss: 0.2918 | Train Acc: 80.95%\n",
      " 59%|██████████████████████▎               | 47/80 [2:19:33<1:35:50, 174.25s/it]Epoch: 48 *****\n",
      "Train Loss: 0.2917 | Train Acc: 80.94%\n",
      " 60%|██████████████████████▊               | 48/80 [2:22:26<1:32:43, 173.85s/it]Epoch: 49 *****\n",
      "Train Loss: 0.2917 | Train Acc: 80.93%\n",
      " 61%|███████████████████████▎              | 49/80 [2:25:19<1:29:44, 173.71s/it]Epoch: 50 *****\n",
      "Train Loss: 0.2918 | Train Acc: 80.93%\n",
      " 62%|███████████████████████▊              | 50/80 [2:28:13<1:26:49, 173.64s/it]Epoch: 51 *****\n",
      "Train Loss: 0.2913 | Train Acc: 80.97%\n",
      " 64%|████████████████████████▏             | 51/80 [2:31:06<1:23:57, 173.70s/it]Epoch: 52 *****\n",
      "Train Loss: 0.2921 | Train Acc: 80.95%\n",
      " 65%|████████████████████████▋             | 52/80 [2:34:01<1:21:09, 173.92s/it]Epoch: 53 *****\n",
      "Train Loss: 0.2912 | Train Acc: 80.95%\n",
      " 66%|█████████████████████████▏            | 53/80 [2:36:54<1:18:13, 173.85s/it]Epoch: 54 *****\n",
      "Train Loss: 0.2909 | Train Acc: 80.97%\n",
      " 68%|█████████████████████████▋            | 54/80 [2:39:48<1:15:15, 173.66s/it]Epoch: 55 *****\n",
      "Train Loss: 0.2914 | Train Acc: 80.92%\n",
      " 69%|██████████████████████████▏           | 55/80 [2:42:41<1:12:20, 173.60s/it]Epoch: 56 *****\n",
      "Train Loss: 0.2898 | Train Acc: 80.97%\n",
      " 70%|██████████████████████████▌           | 56/80 [2:45:35<1:09:27, 173.66s/it]Epoch: 57 *****\n",
      "Train Loss: 0.2906 | Train Acc: 80.97%\n",
      " 71%|███████████████████████████           | 57/80 [2:48:28<1:06:30, 173.51s/it]Epoch: 58 *****\n",
      "Train Loss: 0.2904 | Train Acc: 81.00%\n",
      " 72%|███████████████████████████▌          | 58/80 [2:51:21<1:03:34, 173.38s/it]Epoch: 59 *****\n",
      "Train Loss: 0.2902 | Train Acc: 80.99%\n",
      "Early stopping. No improvement in 59 epochs.\n",
      " 72%|███████████████████████████▌          | 58/80 [2:54:14<1:06:05, 180.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# amnet_train\n",
    "!python Atom_matching_network/train/amnet_train_fold.py --fold 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54a7e50a-65d8-4549-b5f3-d11d050ecc64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting localmapper\n",
      "  Downloading localmapper-0.1.5-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.16.4 in ./lib/python3.10/site-packages (from localmapper) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.4 in ./lib/python3.10/site-packages (from localmapper) (3.10.1)\n",
      "Requirement already satisfied: torch>=1.0.0 in ./lib/python3.10/site-packages (from localmapper) (2.0.1+cu117)\n",
      "Collecting dgl>=0.5.2 (from localmapper)\n",
      "  Downloading dgl-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (553 bytes)\n",
      "Collecting dgllife>=0.2.6 (from localmapper)\n",
      "  Downloading dgllife-0.3.2-py3-none-any.whl.metadata (667 bytes)\n",
      "Requirement already satisfied: scipy>=1.1.0 in ./lib/python3.10/site-packages (from dgl>=0.5.2->localmapper) (1.15.2)\n",
      "Requirement already satisfied: networkx>=2.1 in ./lib/python3.10/site-packages (from dgl>=0.5.2->localmapper) (3.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./lib/python3.10/site-packages (from dgl>=0.5.2->localmapper) (2.32.3)\n",
      "Requirement already satisfied: tqdm in ./lib/python3.10/site-packages (from dgl>=0.5.2->localmapper) (4.67.1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in ./lib/python3.10/site-packages (from dgl>=0.5.2->localmapper) (7.0.0)\n",
      "Collecting torchdata>=0.5.0 (from dgl>=0.5.2->localmapper)\n",
      "  Downloading torchdata-0.11.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in ./lib/python3.10/site-packages (from dgllife>=0.2.6->localmapper) (1.6.1)\n",
      "Requirement already satisfied: pandas in ./lib/python3.10/site-packages (from dgllife>=0.2.6->localmapper) (2.2.3)\n",
      "Collecting hyperopt (from dgllife>=0.2.6->localmapper)\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: joblib in ./lib/python3.10/site-packages (from dgllife>=0.2.6->localmapper) (1.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./lib/python3.10/site-packages (from matplotlib>=3.3.4->localmapper) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./lib/python3.10/site-packages (from matplotlib>=3.3.4->localmapper) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./lib/python3.10/site-packages (from matplotlib>=3.3.4->localmapper) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./lib/python3.10/site-packages (from matplotlib>=3.3.4->localmapper) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./lib/python3.10/site-packages (from matplotlib>=3.3.4->localmapper) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in ./lib/python3.10/site-packages (from matplotlib>=3.3.4->localmapper) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./lib/python3.10/site-packages (from matplotlib>=3.3.4->localmapper) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./lib/python3.10/site-packages (from matplotlib>=3.3.4->localmapper) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in ./lib/python3.10/site-packages (from torch>=1.0.0->localmapper) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in ./lib/python3.10/site-packages (from torch>=1.0.0->localmapper) (4.13.2)\n",
      "Requirement already satisfied: sympy in ./lib/python3.10/site-packages (from torch>=1.0.0->localmapper) (1.13.1)\n",
      "Requirement already satisfied: jinja2 in ./lib/python3.10/site-packages (from torch>=1.0.0->localmapper) (3.1.6)\n",
      "Requirement already satisfied: triton==2.0.0 in ./lib/python3.10/site-packages (from torch>=1.0.0->localmapper) (2.0.0)\n",
      "Requirement already satisfied: cmake in ./lib/python3.10/site-packages (from triton==2.0.0->torch>=1.0.0->localmapper) (3.25.0)\n",
      "Requirement already satisfied: lit in ./lib/python3.10/site-packages (from triton==2.0.0->torch>=1.0.0->localmapper) (15.0.7)\n",
      "Requirement already satisfied: six>=1.5 in ./lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->localmapper) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./lib/python3.10/site-packages (from requests>=2.19.0->dgl>=0.5.2->localmapper) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.10/site-packages (from requests>=2.19.0->dgl>=0.5.2->localmapper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./lib/python3.10/site-packages (from requests>=2.19.0->dgl>=0.5.2->localmapper) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.10/site-packages (from requests>=2.19.0->dgl>=0.5.2->localmapper) (2025.1.31)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./lib/python3.10/site-packages (from scikit-learn>=0.22.2->dgllife>=0.2.6->localmapper) (3.6.0)\n",
      "Collecting future (from hyperopt->dgllife>=0.2.6->localmapper)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting cloudpickle (from hyperopt->dgllife>=0.2.6->localmapper)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting py4j (from hyperopt->dgllife>=0.2.6->localmapper)\n",
      "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./lib/python3.10/site-packages (from jinja2->torch>=1.0.0->localmapper) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./lib/python3.10/site-packages (from pandas->dgllife>=0.2.6->localmapper) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./lib/python3.10/site-packages (from pandas->dgllife>=0.2.6->localmapper) (2025.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./lib/python3.10/site-packages (from sympy->torch>=1.0.0->localmapper) (1.3.0)\n",
      "Downloading localmapper-0.1.5-py3-none-any.whl (34.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.7/34.7 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dgl-2.1.0-cp310-cp310-manylinux1_x86_64.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dgllife-0.3.2-py3-none-any.whl (226 kB)\n",
      "Downloading torchdata-0.11.0-py3-none-any.whl (61 kB)\n",
      "Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Installing collected packages: py4j, future, cloudpickle, hyperopt, dgllife, torchdata, dgl, localmapper\n",
      "Successfully installed cloudpickle-3.1.1 dgl-2.1.0 dgllife-0.3.2 future-1.0.0 hyperopt-0.2.7 localmapper-0.1.5 py4j-0.10.9.9 torchdata-0.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install localmapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b298b66-02a6-40c1-827e-7976fd944c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "  warnings.warn(\n",
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n",
      "--------------------\n",
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "✅ Загружено 5 моделей AMNet\n",
      "Loaded LocalMapper (version=202403) at device cpu\n",
      "\n",
      "📊 Средняя точность: 0.9573 (95.73%)\n",
      "Медианная точность: 0.9857\n",
      "Стд. отклонение: 0.0571\n",
      "Мин/Макс: 0.7222 / 1.0000\n"
     ]
    }
   ],
   "source": [
    "!python Atom_matching_network/Test/test_ensemble.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e002fec-1e3c-474c-8985-cf9e4fb5d010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "  warnings.warn(\n",
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n",
      "--------------------\n",
      "/home/alpiano/miniconda3/envs/chem_env/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "cuda\n",
      "FMNet(\n",
      "gnn=GIN(111, 512, num_layers=3, batch_norm=False, cat=True, lin=True),\n",
      "\n",
      "Namespace(num_wl_iterations=3, node_features_dim=111, edge_feature_dim=10, santitize=False, embedding_dim=512, num_layers=3, lr=0.001, batch_size=1)\n",
      "********************\n",
      "Loaded LocalMapper (version=202403) at device cpu\n",
      "Средняя точность модели: 0.9565 (95.65%)\n",
      "Медианная точность: 0.9792\n",
      "Стандартное отклонение: 0.0590\n",
      "Минимальная точность: 0.7222\n",
      "Максимальная точность: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# оценка только реакций, где localmapper не уверен, одиночная модель amnet\n",
    "!python Atom_matching_network/Test/test_batch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c78e54e-87d1-4a95-94b1-5defe1ad0d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
